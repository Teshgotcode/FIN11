{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44071e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 0: IMPORT LIBRARIES\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from linearmodels.panel import PanelOLS\n",
    "from stargazer.stargazer import Stargazer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1: SETUP - DEFINE CONSTANTS AND TICKERS\n",
    "\n",
    "TICKERS = {\n",
    "    \"Large Cap\": [\n",
    "        'EQNR.OL', 'DNB.OL', 'KOG.OL', 'MOWI.OL', 'TEL.OL', 'NHY.OL', 'AKRBP.OL',\n",
    "        'ORK.OL', 'STB.OL', 'YAR.OL', 'SUBC.OL', 'GJF.OL', 'SALM.OL', 'TGS.OL',\n",
    "        'TOM.OL', 'VAR.OL', 'NEL.OL', 'FRO.OL', 'BWLPG.OL', 'HAUTO.OL',\n",
    "        'NOD.OL', 'WAWI.OL', 'NAS.OL', 'BAKKA.OL', 'WWI.OL', 'AFK.OL',\n",
    "        'AUSS.OL', 'SCATC.OL', 'MPCC.OL', 'HAFNI.OL'\n",
    "    ],\n",
    "    \"Mid Cap\": [\n",
    "        'AKER.OL', 'LSG.OL', 'KIT.OL', 'AKSO.OL', 'PARB.OL', 'BONHR.OL',\n",
    "        'BOUV.OL', 'DNO.OL', 'ENTRA.OL', 'FLNG.OL', 'MING.OL', 'NAPA.OL',\n",
    "        'NORBT.OL', 'OLT.OL', 'PCIB.OL', 'REACH.OL', 'WSTEP.OL', 'KOA.OL',\n",
    "        'HSPG.OL', 'SOFF.OL', 'ABG.OL', 'BGBIO.OL', 'EMGS.OL', 'EXTX.OL',\n",
    "        'HAVI.OL', 'HELG.OL', 'IDEX.OL', 'JIN.OL', 'MULTI.OL', 'NYKD.OL'\n",
    "    ],\n",
    "    \"Small Cap\": [\n",
    "        'QEC.OL', 'RECSI.OL', 'SPOL.OL', 'AZT.OL', 'KID.OL', 'SATS.OL',\n",
    "        'AURG.OL', 'PEN.OL', 'LINK.OL', 'PROT.OL', 'IOX.OL', 'ACC.OL',\n",
    "        'TECH.OL', 'CONTX.OL', 'NONG.OL', 'BEWI.OL', 'ELO.OL', 'GSF.OL',\n",
    "        'PRS.OL', 'AIRX.OL', 'OBSRV.OL', 'HUNT.OL', 'AKVA.OL', 'HEX.OL',\n",
    "        'SOFTX.OL', 'ASA.OL', 'NORTH.OL', 'CAPSL.OL', 'LYTIX.OL', 'VOW.OL'\n",
    "    ]\n",
    "}\n",
    "\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "OUTPUT_CSV = \"oslo_bors_labelled_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1.1: CORWIN-SCHULTZ, ABDI-RANALDO, AND ROLL SPREAD ESTIMATORS\n",
    "def corwin_schultz_spread(group):\n",
    "    \"\"\"\n",
    "    Corwin-Schultz (2012) High-Low spread estimator.\n",
    "    This function calculates a daily spread estimate.\n",
    "    \n",
    "    Reference: Corwin, S. A., & Schultz, P. (2012). A simple way to estimate \n",
    "    bid-ask spreads from daily high and low prices. The Journal of Finance, 67(2), 719-760.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "    \n",
    "    high = group['High'].values\n",
    "    low = group['Low'].values\n",
    "    \n",
    "    spreads = []\n",
    "    negative_count = 0\n",
    "    \n",
    "    for i in range(len(group)):\n",
    "        if i == 0:\n",
    "            spreads.append(np.nan)\n",
    "        else:\n",
    "            h0, l0 = high[i], low[i]\n",
    "            h1, l1 = high[i-1], low[i-1]\n",
    "            \n",
    "            if h0 > 0 and l0 > 0 and h1 > 0 and l1 > 0:\n",
    "                beta = (np.log(h0/l0))**2 + (np.log(h1/l1))**2\n",
    "                h_max = max(h0, h1)\n",
    "                l_min = min(l0, l1)\n",
    "                gamma = (np.log(h_max/l_min))**2\n",
    "                \n",
    "                k = (3 - 2*np.sqrt(2))\n",
    "                alpha = (np.sqrt(2*beta) - np.sqrt(beta)) / k - np.sqrt(gamma / k)\n",
    "                \n",
    "                spread = 2 * (np.exp(alpha) - 1) / (1 + np.exp(alpha))\n",
    "                \n",
    "                if spread >= 0:\n",
    "                    spreads.append(spread)\n",
    "                else:\n",
    "                    spreads.append(np.nan)\n",
    "                    negative_count += 1\n",
    "            else:\n",
    "                spreads.append(np.nan)\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        ticker = group['Ticker'].iloc[0] if 'Ticker' in group.columns else 'Unknown'\n",
    "        # print(f\"  Warning: {ticker} had {negative_count} negative Corwin-Schultz estimates (set to NaN)\")\n",
    "    \n",
    "    group['Spread'] = spreads\n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "def abdi_ranaldo_spread(group):\n",
    "    \"\"\"\n",
    "    Calculates the NORMALIZED Abdi and Ranaldo (2017) \"BAR\" spread estimator.\n",
    "    This version converts the absolute spread (in currency) to a relative \n",
    "    spread (as a percentage of the price), making it comparable across stocks.\n",
    "    \n",
    "    Note: Additional filtering is applied to remove economically meaningless spreads.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "\n",
    "    delta_p_t = group['Close'].diff()\n",
    "    delta_p_t_minus_1 = delta_p_t.shift(1)\n",
    "\n",
    "    if delta_p_t.count() < 2:\n",
    "        return pd.Series(np.nan, index=group.index, name='Spread')\n",
    "\n",
    "    cov_term = (delta_p_t * delta_p_t_minus_1)\n",
    "    \n",
    "    # Calculate the absolute spread (in currency units)\n",
    "    absolute_spreads = 2 * np.sqrt(np.maximum(0, -cov_term))\n",
    "    \n",
    "    # NORMALIZATION STEP\n",
    "    # To get a relative spread, divide by the average of today's and yesterday's close price.\n",
    "    # Rolling window to get the average price at each point in time.\n",
    "    avg_price = group['Close'].rolling(window=2, min_periods=1).mean()\n",
    "    \n",
    "    # Calculate the relative spread. Use .values to avoid index alignment issues.\n",
    "    relative_spreads = absolute_spreads / avg_price.values\n",
    "    \n",
    "    # CRITICAL FIX: Filter out economically meaningless spreads\n",
    "    # Spreads below 0.01% (0.0001) are likely measurement errors or rounding artifacts\n",
    "    relative_spreads = np.where(relative_spreads < 0.0001, np.nan, relative_spreads)\n",
    "    \n",
    "    # Assign the comparable, relative spreads to the DataFrame\n",
    "    group['Spread'] = relative_spreads\n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "def get_roll_spread(group):\n",
    "    \"\"\"\n",
    "    Roll (1984) spread estimator based on serial covariance of price changes.\n",
    "    Returns a single spread estimate per stock (not time-varying).\n",
    "    \n",
    "    Roll's spread = 2 * sqrt(-Cov(ŒîP_t, ŒîP_t-1))\n",
    "    \n",
    "    The Roll estimator assumes that negative serial covariance in returns is \n",
    "    caused by bid-ask bounce. If the covariance is positive, it violates \n",
    "    Roll's model assumptions and we return NaN.\n",
    "    \n",
    "    Reference: Roll, R. (1984). A simple implicit measure of the effective \n",
    "    bid-ask spread in an efficient market. The Journal of Finance, 39(4), 1127-1139.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "    returns = group['Close'].pct_change().dropna()\n",
    "    \n",
    "    if len(returns) < 30:  # Minimum observations for reliable estimate\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate serial covariance (covariance between returns and lagged returns)\n",
    "    cov = returns.cov(returns.shift(1))\n",
    "    \n",
    "    # Roll's spread formula (only valid when covariance is negative)\n",
    "    if pd.notna(cov) and cov < 0:\n",
    "        spread = 2 * np.sqrt(-cov)\n",
    "    else:\n",
    "        # Positive covariance violates Roll's assumptions\n",
    "        spread = np.nan\n",
    "    \n",
    "    return spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2522e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1.2: ADD PERIOD LABELS FOR HOLIDAY ANALYSIS\n",
    "def add_period_labels(df):\n",
    "    \"\"\"\n",
    "    Adds period labels for holiday analysis.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    years = range(df['Date'].dt.year.min(), df['Date'].dt.year.max() + 1)\n",
    "    norway_holidays = holidays.Norway(years=years)\n",
    "    \n",
    "    df['week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['period_label'] = 'control_year'\n",
    "\n",
    "    for yr in years:\n",
    "        christmas_ref_start = pd.Timestamp(f'{yr}-12-15')\n",
    "        christmas_ref_end = pd.Timestamp(f'{yr}-12-31')\n",
    "        christmas_days = pd.bdate_range(start=christmas_ref_start, end=christmas_ref_end, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "        \n",
    "        if len(christmas_days) > 0:\n",
    "            pre_christmas = pd.bdate_range(end=christmas_days[0] - pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            try:\n",
    "                post_christmas_start = pd.Timestamp(f'{yr+1}-01-02')\n",
    "                post_christmas = pd.bdate_range(start=post_christmas_start, periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            except:\n",
    "                post_christmas = pd.bdate_range(start=christmas_days[-1] + pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            \n",
    "            df.loc[df['Date'].isin(pre_christmas), 'period_label'] = 'pre_christmas'\n",
    "            df.loc[df['Date'].isin(christmas_days), 'period_label'] = 'christmas'\n",
    "            df.loc[df['Date'].isin(post_christmas), 'period_label'] = 'post_christmas'\n",
    "\n",
    "    for yr in years:\n",
    "        easter_days = [d for d, h in norway_holidays.items() if ('P√•ske' in h or h in ['Skj√¶rtorsdag', 'Langfredag']) and d.year == yr]\n",
    "        \n",
    "        if easter_days:\n",
    "            easter_start = min(easter_days)\n",
    "            easter_end = max(easter_days)\n",
    "            pre_easter = pd.bdate_range(end=easter_start - pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            post_easter = pd.bdate_range(start=easter_end + pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            df.loc[df['Date'].isin(pre_easter), 'period_label'] = 'pre_easter'\n",
    "            df.loc[df['Date'].isin(post_easter), 'period_label'] = 'post_easter'\n",
    "\n",
    "    df.loc[df['week'].between(28, 30), 'period_label'] = 'summer_holiday'\n",
    "    df.loc[(df['month'].between(6, 8)) & (~df['week'].between(28, 30)), 'period_label'] = 'summer_excl_holiday'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dba716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analysis will use the 'Corwin-Schultz (2012) Spread'\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 1.3: CHOOSE SPREAD ESTIMATOR (Simplified)\n",
    "user_input = input(\"Which spread estimator do you want to use? (Enter 'CS' or 'AR'): \")\n",
    "\n",
    "# Determine the spread method based on user input\n",
    "if user_input.upper() == 'AR':\n",
    "    METHOD_CHOICE = 'AR'\n",
    "    SPREAD_METHOD_NAME = 'Abdi & Ranaldo (2017) Spread'\n",
    "else:\n",
    "    METHOD_CHOICE = 'CS'\n",
    "    SPREAD_METHOD_NAME = 'Corwin-Schultz (2012) Spread'\n",
    "\n",
    "print(f\"The analysis will use the '{SPREAD_METHOD_NAME}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010d145d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "OSLO B√òRS HOLIDAY EFFECTS ANALYSIS - DATA PREPARATION\n",
      "Using spread estimator: Corwin-Schultz (2012) Spread\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Downloading data for 90 tickers...\n",
      "Date range: 2014-01-01 to 2024-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  90 of 90 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRANSFORMING DATA STRUCTURE\n",
      "--------------------------------------------------------------------------------\n",
      "Removed 33,216 rows with missing price/volume data\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CALCULATING CORWIN-SCHULTZ SPREADS (DAILY ESTIMATE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/whd5k2z14l1f9v052bj7zhfm0000gn/T/ipykernel_16338/968052951.py:36: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Ticker', group_keys=False).apply(corwin_schultz_spread)\n",
      "/var/folders/4f/whd5k2z14l1f9v052bj7zhfm0000gn/T/ipykernel_16338/968052951.py:119: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  roll_spreads = df.groupby('Ticker').apply(get_roll_spread)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread calculation complete.\n",
      "\n",
      "================================================================================\n",
      "üìä SPREAD DISTRIBUTION DIAGNOSTICS - CS\n",
      "================================================================================\n",
      "\n",
      "Spread Value Distribution:\n",
      "  Total observations:        215,364\n",
      "  Zero values (= 0):         11,986 (5.57%)\n",
      "  Negative values (< 0):     0 (0.00%)\n",
      "  Tiny spreads (0 to 0.01%): 1,215 (0.56%)\n",
      "  Valid spreads (‚â• 0.01%):   119,138 (55.32%)\n",
      "  Missing (NaN):             83,025 (38.55%)\n",
      "\n",
      "Spread Percentiles (before filtering):\n",
      "    min: 0.000000\n",
      "     1%: 0.000000\n",
      "     5%: 0.000000\n",
      "    10%: 0.000108\n",
      "    25%: 0.004321\n",
      "    50%: 0.010319\n",
      "    75%: 0.019523\n",
      "    90%: 0.033346\n",
      "    95%: 0.045516\n",
      "    99%: 0.080000\n",
      "    max: 0.408711\n",
      "\n",
      "‚úì Median spread: 0.010319 (appears reasonable)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATA QUALITY FILTERING - CS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Step 1: Removed 83,025 rows with NaN spreads\n",
      "Step 2: Removed 13,201 rows with spreads < 0.01% (1 bp)\n",
      "        ‚Üí These are economically meaningless and likely measurement errors\n",
      "\n",
      "Total rows removed: 96,226\n",
      "Remaining observations: 119,138\n",
      "\n",
      "üìä POST-FILTERING SPREAD STATISTICS:\n",
      "  Mean:   0.016541\n",
      "  Median: 0.011741\n",
      "  Std:    0.017238\n",
      "  Min:    0.000100\n",
      "  Max:    0.408711\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CALCULATING ROLL'S SPREADS (VALIDATION)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Roll's spread calculation summary:\n",
      "  Tickers with valid spread: 64 / 90\n",
      "  Tickers with invalid spread: 26\n",
      "    ‚Üí Likely due to positive serial covariance (violates Roll's assumptions)\n",
      "  Average Roll's spread: 0.019814\n",
      "\n",
      "‚úì Correlation between Roll and CS average: 0.672\n",
      "  ‚Üí Moderate correlation - spread estimates are reasonably consistent\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ADDING PERIOD LABELS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FINAL DATA EXPORT\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Data saved to: oslo_bors_labelled_data.csv\n",
      "Total observations: 119,138\n",
      "Date range: 2014-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "Tickers: 90\n",
      "\n",
      "üìä FINAL SPREAD SUMMARY BY CAP GROUP:\n",
      "           Observations      Mean    Median   Std Dev\n",
      "Cap_Group                                            \n",
      "Large Cap         41855  0.012159  0.009567  0.010714\n",
      "Mid Cap           41032  0.017079  0.012009  0.017817\n",
      "Small Cap         36251  0.020989  0.015222  0.021036\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "‚úì DATA PROCESSING COMPLETE\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 1.4: MAIN DATA PROCESSING FUNCTION\n",
    "def main():\n",
    "    print(\"-\" * 80)\n",
    "    print(\"OSLO B√òRS HOLIDAY EFFECTS ANALYSIS - DATA PREPARATION\")\n",
    "    # Use f-string to print the full name\n",
    "    print(f\"Using spread estimator: {SPREAD_METHOD_NAME}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    all_tickers = [ticker for sublist in TICKERS.values() for ticker in sublist]\n",
    "    print(f\"\\nDownloading data for {len(all_tickers)} tickers...\")\n",
    "    print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "    \n",
    "    raw_data = yf.download(all_tickers, start=START_DATE, end=END_DATE, progress=True, auto_adjust=True)\n",
    "\n",
    "    if raw_data.empty:\n",
    "        print(\"ERROR: No data downloaded\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRANSFORMING DATA STRUCTURE\")\n",
    "    print(\"-\"*80)\n",
    "    df = raw_data.stack(future_stack=True).reset_index()\n",
    "    df = df.rename(columns={'level_1': 'Ticker'})\n",
    "    \n",
    "    ticker_to_cap = {ticker: cap for cap, tickers in TICKERS.items() for ticker in tickers}\n",
    "    df['Cap_Group'] = df['Ticker'].map(ticker_to_cap)\n",
    "    \n",
    "    initial_len = len(df)\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume', 'Cap_Group'], inplace=True)\n",
    "    print(f\"Removed {initial_len - len(df):,} rows with missing price/volume data\")\n",
    "\n",
    "    # Spread Calculation\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    if METHOD_CHOICE == 'CS':\n",
    "        print(\"CALCULATING CORWIN-SCHULTZ SPREADS (DAILY ESTIMATE)\")\n",
    "        df = df.groupby('Ticker', group_keys=False).apply(corwin_schultz_spread)\n",
    "    elif METHOD_CHOICE == 'AR':\n",
    "        print(\"CALCULATING ABDI & RANALDO (2017) 'BAR' SPREADS\")\n",
    "        df = df.groupby('Ticker', group_keys=False).apply(abdi_ranaldo_spread)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method choice. Please choose 'CS' or 'AR'.\")\n",
    "    print(\"Spread calculation complete.\")\n",
    "\n",
    "    # CRITICAL FIX: COMPREHENSIVE SPREAD DIAGNOSTICS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üìä SPREAD DISTRIBUTION DIAGNOSTICS - {METHOD_CHOICE}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count spread values by category\n",
    "    total_obs = len(df)\n",
    "    zero_spreads = (df['Spread'] == 0).sum()\n",
    "    negative_spreads = (df['Spread'] < 0).sum()\n",
    "    tiny_spreads = ((df['Spread'] > 0) & (df['Spread'] < 0.0001)).sum()\n",
    "    valid_spreads = (df['Spread'] >= 0.0001).sum()\n",
    "    nan_spreads = df['Spread'].isna().sum()\n",
    "    \n",
    "    print(f\"\\nSpread Value Distribution:\")\n",
    "    print(f\"  Total observations:        {total_obs:,}\")\n",
    "    print(f\"  Zero values (= 0):         {zero_spreads:,} ({zero_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Negative values (< 0):     {negative_spreads:,} ({negative_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Tiny spreads (0 to 0.01%): {tiny_spreads:,} ({tiny_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Valid spreads (‚â• 0.01%):   {valid_spreads:,} ({valid_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Missing (NaN):             {nan_spreads:,} ({nan_spreads/total_obs*100:.2f}%)\")\n",
    "    \n",
    "    # Percentile analysis\n",
    "    print(f\"\\nSpread Percentiles (before filtering):\")\n",
    "    percentiles = df['Spread'].describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99])\n",
    "    for stat in ['min', '1%', '5%', '10%', '25%', '50%', '75%', '90%', '95%', '99%', 'max']:\n",
    "        if stat in percentiles.index:\n",
    "            print(f\"  {stat:>5}: {percentiles[stat]:.6f}\")\n",
    "    \n",
    "    # Warning if median is zero or very low\n",
    "    median_spread = df['Spread'].median()\n",
    "    if median_spread == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Median spread is ZERO!\")\n",
    "        print(\"   ‚Üí This indicates a data quality issue with the spread estimator.\")\n",
    "        print(\"   ‚Üí Most observations have zero spread, which is economically unrealistic.\")\n",
    "    elif median_spread < 0.0001:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Median spread is very low ({median_spread:.6f})\")\n",
    "        print(\"   ‚Üí Many spreads may be economically meaningless.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Median spread: {median_spread:.6f} (appears reasonable)\")\n",
    "\n",
    "    # Data Quality Check and Cleaning\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"DATA QUALITY FILTERING - {METHOD_CHOICE}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    spread_before = len(df)\n",
    "    \n",
    "    # Remove NaN spreads\n",
    "    df = df.dropna(subset=['Spread'])\n",
    "    nan_removed = spread_before - len(df)\n",
    "    print(f\"\\nStep 1: Removed {nan_removed:,} rows with NaN spreads\")\n",
    "    \n",
    "    # CRITICAL FIX: Remove economically meaningless spreads\n",
    "    # Spreads below 0.01% (1 basis point) are likely measurement errors\n",
    "    spread_before_filter = len(df)\n",
    "    df = df[df['Spread'] >= 0.0001]\n",
    "    tiny_removed = spread_before_filter - len(df)\n",
    "    print(f\"Step 2: Removed {tiny_removed:,} rows with spreads < 0.01% (1 bp)\")\n",
    "    print(f\"        ‚Üí These are economically meaningless and likely measurement errors\")\n",
    "    \n",
    "    print(f\"\\nTotal rows removed: {spread_before - len(df):,}\")\n",
    "    print(f\"Remaining observations: {len(df):,}\")\n",
    "    \n",
    "    # Post-filtering diagnostics\n",
    "    print(f\"\\nüìä POST-FILTERING SPREAD STATISTICS:\")\n",
    "    print(f\"  Mean:   {df['Spread'].mean():.6f}\")\n",
    "    print(f\"  Median: {df['Spread'].median():.6f}\")\n",
    "    print(f\"  Std:    {df['Spread'].std():.6f}\")\n",
    "    print(f\"  Min:    {df['Spread'].min():.6f}\")\n",
    "    print(f\"  Max:    {df['Spread'].max():.6f}\")\n",
    "\n",
    "    # Roll's Spread Calculation\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"CALCULATING ROLL'S SPREADS (VALIDATION)\")\n",
    "    print(\"-\" * 80)\n",
    "    roll_spreads = df.groupby('Ticker').apply(get_roll_spread)\n",
    "    df['RollsSpread'] = df['Ticker'].map(roll_spreads)\n",
    "    \n",
    "    valid_rolls = roll_spreads.notna().sum()\n",
    "    total_tickers = df['Ticker'].nunique()\n",
    "    invalid_rolls = total_tickers - valid_rolls\n",
    "    \n",
    "    print(f\"\\nRoll's spread calculation summary:\")\n",
    "    print(f\"  Tickers with valid spread: {valid_rolls} / {total_tickers}\")\n",
    "    if invalid_rolls > 0:\n",
    "        print(f\"  Tickers with invalid spread: {invalid_rolls}\")\n",
    "        print(f\"    ‚Üí Likely due to positive serial covariance (violates Roll's assumptions)\")\n",
    "    print(f\"  Average Roll's spread: {roll_spreads.mean():.6f}\")\n",
    "    \n",
    "    # Compare Roll with CS/AR\n",
    "    avg_cs_ar = df.groupby('Ticker')['Spread'].mean()\n",
    "    comparison = pd.DataFrame({\n",
    "        'Roll': roll_spreads,\n",
    "        'CS_AR_Avg': avg_cs_ar\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(comparison) > 0:\n",
    "        correlation = comparison.corr().iloc[0, 1]\n",
    "        print(f\"\\n‚úì Correlation between Roll and {METHOD_CHOICE} average: {correlation:.3f}\")\n",
    "        if correlation > 0.7:\n",
    "            print(f\"  ‚Üí Strong correlation indicates reliable spread estimates!\")\n",
    "        elif correlation > 0.5:\n",
    "            print(f\"  ‚Üí Moderate correlation - spread estimates are reasonably consistent\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Low correlation - may indicate measurement issues\")\n",
    "\n",
    "    # Period Labeling and Final Export\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"ADDING PERIOD LABELS\")\n",
    "    print(\"-\"*80)\n",
    "    df = add_period_labels(df)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"FINAL DATA EXPORT\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    final_cols = ['Date', 'Ticker', 'Cap_Group', 'Open', 'High', 'Low', 'Close', \n",
    "                  'Volume', 'Spread', 'RollsSpread', 'period_label']\n",
    "    df_final = df[[col for col in final_cols if col in df.columns]]\n",
    "    df_final.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\nData saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Total observations: {len(df_final):,}\")\n",
    "    print(f\"Date range: {df_final['Date'].min()} to {df_final['Date'].max()}\")\n",
    "    print(f\"Tickers: {df_final['Ticker'].nunique()}\")\n",
    "    \n",
    "    # Final summary by cap group\n",
    "    print(f\"\\nüìä FINAL SPREAD SUMMARY BY CAP GROUP:\")\n",
    "    summary = df_final.groupby('Cap_Group')['Spread'].agg(['count', 'mean', 'median', 'std'])\n",
    "    summary.columns = ['Observations', 'Mean', 'Median', 'Std Dev']\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"‚úì DATA PROCESSING COMPLETE\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e25dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DESCRIPTIVE STATISTICS AND VISUALIZATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total observations: 119,138\n",
      "Date range: 2014-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "Unique tickers: 90\n",
      "Trading days: 2,757\n",
      "\n",
      "================================================================================\n",
      "EFFECT SIZE ANALYSIS (COHEN'S D)\n",
      "================================================================================\n",
      "\n",
      "Cohen's d Effect Sizes (vs. Control Period):\n",
      "  Summer Holiday:    d = -0.1261\n",
      "  Christmas:         d = 0.0268\n",
      "  Pre-Easter:        d = -0.0011\n",
      "  Post-Easter:       d = -0.0301\n",
      "\n",
      "Interpretation: |d| > 0.5 = medium to large effect\n",
      "\n",
      "================================================================================\n",
      "HARRIS FRAMEWORK: DEALER BEHAVIOR HYPOTHESIS\n",
      "================================================================================\n",
      "\n",
      "Testing Harris's prediction: Spreads should increase when volume decreases\n",
      "(Lower dealer participation and higher inventory risk)\n",
      "\n",
      "Spread by Volume Quartile:\n",
      "                 Spread_Mean  Spread_Median  N_Obs    Avg_Volume\n",
      "Volume_Quartile                                                 \n",
      "Q1_Lowest           0.017639       0.011911  29785  7.352360e+03\n",
      "Q2_Low              0.017098       0.012185  29784  6.796623e+04\n",
      "Q3_High             0.015510       0.011719  29784  3.030757e+05\n",
      "Q4_Highest          0.015914       0.011193  29785  2.579836e+06\n",
      "\n",
      "Harris Hypothesis Test:\n",
      "  H0: Spread(Low Volume) = Spread(High Volume)\n",
      "  H1: Spread(Low Volume) > Spread(High Volume)\n",
      "  Mean spread Q1 (low vol):  0.017639\n",
      "  Mean spread Q4 (high vol): 0.015914\n",
      "  Difference: 0.001725\n",
      "  t-statistic: 11.5190\n",
      "  p-value: 0.0000\n",
      "  Cohen's d: 0.0944\n",
      "  ‚úì RESULT: Harris's prediction CONFIRMED (p < 0.05)\n",
      "    ‚Üí Spreads are wider when volume is low (dealer participation reduced)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SUMMARY BY CAP GROUP\n",
      "--------------------------------------------------------------------------------\n",
      "           Spread_Mean  Spread_Median  Spread_Std   Volume_Mean  \\\n",
      "Cap_Group                                                         \n",
      "Large Cap     0.012159       0.009567    0.010714  1.276429e+06   \n",
      "Mid Cap       0.017079       0.012009    0.017817  4.920147e+05   \n",
      "Small Cap     0.020989       0.015222    0.021036  3.999127e+05   \n",
      "\n",
      "           Volume_Median  N_Stocks  \n",
      "Cap_Group                           \n",
      "Large Cap       496458.0        30  \n",
      "Mid Cap          58792.0        30  \n",
      "Small Cap        57289.0        30  \n",
      "\n",
      "üìñ HARRIS INTERPRETATION:\n",
      "   Large Cap stocks have narrower spreads due to:\n",
      "   ‚Ä¢ Higher competitive market making (Harris Chapter 6)\n",
      "   ‚Ä¢ Lower inventory risk for dealers\n",
      "   ‚Ä¢ Greater participation rates (Harris Chapter 21)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SUMMARY BY PERIOD\n",
      "--------------------------------------------------------------------------------\n",
      "                     Spread_Mean  Spread_Median    Volume_Mean  Volume_Median  \\\n",
      "period_label                                                                    \n",
      "christmas               0.017328       0.011989  771702.221317       137108.0   \n",
      "control_year            0.016855       0.011908  773297.458314       141871.0   \n",
      "post_christmas          0.017227       0.012138  790388.158046       132555.0   \n",
      "post_easter             0.016325       0.012129  695939.587259       147581.0   \n",
      "pre_christmas           0.016813       0.011643  742831.956963       144002.0   \n",
      "pre_easter              0.016835       0.012034  676584.169983       128714.5   \n",
      "summer_excl_holiday     0.015876       0.011386  684904.034218       128248.0   \n",
      "summer_holiday          0.014668       0.011084  569321.676574       105160.0   \n",
      "\n",
      "                     N_Obs  \n",
      "period_label                \n",
      "christmas             4663  \n",
      "control_year         74257  \n",
      "post_christmas        2088  \n",
      "post_easter           2229  \n",
      "pre_christmas         2463  \n",
      "pre_easter            2312  \n",
      "summer_excl_holiday  23993  \n",
      "summer_holiday        7133  \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "CHANGES VS CONTROL PERIOD\n",
      "--------------------------------------------------------------------------------\n",
      "                     Spread_Mean  Spread_Change_%    Volume_Mean  \\\n",
      "period_label                                                       \n",
      "christmas               0.017328             2.81  771702.221317   \n",
      "control_year            0.016855             0.00  773297.458314   \n",
      "post_christmas          0.017227             2.21  790388.158046   \n",
      "post_easter             0.016325            -3.14  695939.587259   \n",
      "pre_christmas           0.016813            -0.25  742831.956963   \n",
      "pre_easter              0.016835            -0.12  676584.169983   \n",
      "summer_excl_holiday     0.015876            -5.81  684904.034218   \n",
      "summer_holiday          0.014668           -12.98  569321.676574   \n",
      "\n",
      "                     Volume_Change_%  \n",
      "period_label                          \n",
      "christmas                      -0.21  \n",
      "control_year                    0.00  \n",
      "post_christmas                  2.21  \n",
      "post_easter                   -10.00  \n",
      "pre_christmas                  -3.94  \n",
      "pre_easter                    -12.51  \n",
      "summer_excl_holiday           -11.43  \n",
      "summer_holiday                -26.38  \n",
      "\n",
      "================================================================================\n",
      "PRACTICAL IMPLICATIONS: TRADING COST ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Spread difference (Summer vs. Control): -0.002187\n",
      "Average stock price assumed: 100 NOK\n",
      "\n",
      "Cost Impact per Trade (one-way):\n",
      "    Trade Size (NOK)    Control Cost     Summer Cost         Savings\n",
      "----------------------------------------------------------------------\n",
      "             100,000         842.75 NOK         733.40 NOK         109.35 NOK\n",
      "             500,000        4213.75 NOK        3667.00 NOK         546.75 NOK\n",
      "           1,000,000        8427.50 NOK        7334.00 NOK        1093.50 NOK\n",
      "           5,000,000       42137.50 NOK       36670.00 NOK        5467.50 NOK\n",
      "\n",
      "üí° INTERPRETATION:\n",
      "   Trading during summer holidays saves approximately 13.0%\n",
      "   on transaction costs due to narrower spreads.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GENERATING FIGURE 1: CHRISTMAS & EASTER\n",
      "--------------------------------------------------------------------------------\n",
      "Saved: Figure_1A_CE_Spread.png\n",
      "Saved: Figure_1B_CE_Volume.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GENERATING FIGURE 2: SUMMER HOLIDAY\n",
      "--------------------------------------------------------------------------------\n",
      "Saved: Figure_2A_Summer_Spread.png\n",
      "Saved: Figure_2B_Summer_Volume.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GENERATING FIGURE 3: P-VALUE HEATMAPS (WITH MULTIPLE CORRECTIONS)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Applying Bonferroni Correction ---\n",
      "Total tests performed: 15\n",
      "Bonferroni-corrected alpha level: 0.00333\n",
      "\n",
      "--- Applying Benjamini-Hochberg (FDR) Correction ---\n",
      "FDR correction applied.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON OF CORRECTED P-VALUES (SPREAD)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Bonferroni Corrected P-Values (Spread) ---\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas\n",
      "Large Cap      0.2127       1.0000            1.0     1.0000             1.0\n",
      "Mid Cap        0.6377       1.0000            1.0     0.2859             1.0\n",
      "Small Cap      1.0000       0.0727            1.0     1.0000             1.0\n",
      "\n",
      "--- FDR Corrected P-Values (Spread) ---\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas\n",
      "Large Cap      0.0953       0.7232         0.3361     0.6677          0.8111\n",
      "Mid Cap        0.1594       0.8111         0.6677     0.0953          0.9687\n",
      "Small Cap      0.8111       0.0727         0.8981     0.7730          0.5127\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON OF CORRECTED P-VALUES (VOLUME)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Bonferroni Corrected P-Values (Volume) ---\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas\n",
      "Large Cap      0.0468       1.0000            1.0     1.0000             1.0\n",
      "Mid Cap        1.0000       1.0000            1.0     1.0000             1.0\n",
      "Small Cap      0.1295       0.9435            1.0     0.9358             1.0\n",
      "\n",
      "--- FDR Corrected P-Values (Volume) ---\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas\n",
      "Large Cap      0.0468       0.4419         0.3647     0.3712          0.3708\n",
      "Mid Cap        0.8324       0.3708         0.8575     0.8249          0.3708\n",
      "Small Cap      0.0647       0.2359         0.4419     0.2359          0.9390\n",
      "\n",
      "Saved: Figure_3A_PValues_Spread.png\n",
      "Saved: Figure_3B_PValues_Volume.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GENERATING FIGURE 4: HEATMAP\n",
      "--------------------------------------------------------------------------------\n",
      "Saved: Figure_4_Heatmap_Spread.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GENERATING FIGURE 5: TIME SERIES | Corwin-Schultz (2012) Spread\n",
      "--------------------------------------------------------------------------------\n",
      "Saved: Figure_5_Timeseries_Monthly.png\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPLETE - 5 essential figures created\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "All outputs saved to 'output/' folder\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 2: DESCRIPTIVE STATISTICS AND VISUALIZATION (ENHANCED)\n",
    "\n",
    "df = pd.read_csv('oslo_bors_labelled_data.csv', parse_dates=['Date'])\n",
    "\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"DESCRIPTIVE STATISTICS AND VISUALIZATION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Dataset overview\n",
    "print(f\"\\nTotal observations: {len(df):,}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Unique tickers: {df['Ticker'].nunique()}\")\n",
    "print(f\"Trading days: {df['Date'].nunique():,}\")\n",
    "\n",
    "# CRITICAL ADDITION: COHEN'S D EFFECT SIZE CALCULATION\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFECT SIZE ANALYSIS (COHEN'S D)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def calculate_cohens_d(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d effect size.\n",
    "    \n",
    "    Interpretation:\n",
    "    - d < 0.2: Small effect\n",
    "    - d = 0.2-0.5: Small to medium effect\n",
    "    - d = 0.5-0.8: Medium to large effect\n",
    "    - d > 0.8: Large effect\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = (group1.mean() - group2.mean()) / pooled_std\n",
    "    \n",
    "    return d\n",
    "\n",
    "# Calculate effect sizes for key holiday periods\n",
    "control = df[df['period_label'] == 'control_year']['Spread']\n",
    "summer = df[df['period_label'] == 'summer_holiday']['Spread']\n",
    "christmas = df[df['period_label'] == 'christmas']['Spread']\n",
    "easter_pre = df[df['period_label'] == 'pre_easter']['Spread']\n",
    "easter_post = df[df['period_label'] == 'post_easter']['Spread']\n",
    "\n",
    "print(\"\\nCohen's d Effect Sizes (vs. Control Period):\")\n",
    "print(f\"  Summer Holiday:    d = {calculate_cohens_d(summer, control):.4f}\")\n",
    "print(f\"  Christmas:         d = {calculate_cohens_d(christmas, control):.4f}\")\n",
    "print(f\"  Pre-Easter:        d = {calculate_cohens_d(easter_pre, control):.4f}\")\n",
    "print(f\"  Post-Easter:       d = {calculate_cohens_d(easter_post, control):.4f}\")\n",
    "print(\"\\nInterpretation: |d| > 0.5 = medium to large effect\")\n",
    "\n",
    "# CRITICAL ADDITION: HARRIS FRAMEWORK - DEALER BEHAVIOR HYPOTHESIS TEST\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HARRIS FRAMEWORK: DEALER BEHAVIOR HYPOTHESIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTesting Harris's prediction: Spreads should increase when volume decreases\")\n",
    "print(\"(Lower dealer participation and higher inventory risk)\")\n",
    "\n",
    "# Create volume quartiles\n",
    "df['Volume_Quartile'] = pd.qcut(df['Volume'], q=4, labels=['Q1_Lowest', 'Q2_Low', 'Q3_High', 'Q4_Highest'], duplicates='drop')\n",
    "\n",
    "# Spread by volume quartile\n",
    "spread_by_volume = df.groupby('Volume_Quartile', observed=True).agg({\n",
    "    'Spread': ['mean', 'median', 'count'],\n",
    "    'Volume': 'mean'\n",
    "}).round(6)\n",
    "spread_by_volume.columns = ['Spread_Mean', 'Spread_Median', 'N_Obs', 'Avg_Volume']\n",
    "\n",
    "print(\"\\nSpread by Volume Quartile:\")\n",
    "print(spread_by_volume)\n",
    "\n",
    "# Statistical test: Is spread in Q1 (lowest volume) significantly higher than Q4?\n",
    "q1_spread = df[df['Volume_Quartile'] == 'Q1_Lowest']['Spread']\n",
    "q4_spread = df[df['Volume_Quartile'] == 'Q4_Highest']['Spread']\n",
    "t_stat, p_val = ttest_ind(q1_spread.dropna(), q4_spread.dropna(), equal_var=False)\n",
    "cohens_d_volume = calculate_cohens_d(q1_spread.dropna(), q4_spread.dropna())\n",
    "\n",
    "print(f\"\\nHarris Hypothesis Test:\")\n",
    "print(f\"  H0: Spread(Low Volume) = Spread(High Volume)\")\n",
    "print(f\"  H1: Spread(Low Volume) > Spread(High Volume)\")\n",
    "print(f\"  Mean spread Q1 (low vol):  {q1_spread.mean():.6f}\")\n",
    "print(f\"  Mean spread Q4 (high vol): {q4_spread.mean():.6f}\")\n",
    "print(f\"  Difference: {q1_spread.mean() - q4_spread.mean():.6f}\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_val:.4f}\")\n",
    "print(f\"  Cohen's d: {cohens_d_volume:.4f}\")\n",
    "\n",
    "if p_val < 0.05 and q1_spread.mean() > q4_spread.mean():\n",
    "    print(\"  ‚úì RESULT: Harris's prediction CONFIRMED (p < 0.05)\")\n",
    "    print(\"    ‚Üí Spreads are wider when volume is low (dealer participation reduced)\")\n",
    "elif p_val < 0.05 and q1_spread.mean() < q4_spread.mean():\n",
    "    print(\"  ‚úó RESULT: Harris's prediction CONTRADICTED (p < 0.05)\")\n",
    "    print(\"    ‚Üí Modern electronic markets may behave differently than Harris predicted\")\n",
    "else:\n",
    "    print(\"  ‚Üí RESULT: No significant relationship (p >= 0.05)\")\n",
    "\n",
    "# Summary by cap group\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SUMMARY BY CAP GROUP\")\n",
    "print(\"-\"*80)\n",
    "summary_cap = df.groupby('Cap_Group').agg({\n",
    "    'Spread': ['mean', 'median', 'std'],\n",
    "    'Volume': ['mean', 'median'],\n",
    "    'Ticker': 'nunique'\n",
    "}).round(6)\n",
    "summary_cap.columns = ['Spread_Mean', 'Spread_Median', 'Spread_Std', 'Volume_Mean', 'Volume_Median', 'N_Stocks']\n",
    "print(summary_cap)\n",
    "\n",
    "# HARRIS INTERPRETATION\n",
    "print(\"\\nüìñ HARRIS INTERPRETATION:\")\n",
    "print(\"   Large Cap stocks have narrower spreads due to:\")\n",
    "print(\"   ‚Ä¢ Higher competitive market making (Harris Chapter 6)\")\n",
    "print(\"   ‚Ä¢ Lower inventory risk for dealers\")\n",
    "print(\"   ‚Ä¢ Greater participation rates (Harris Chapter 21)\")\n",
    "\n",
    "# Summary by period\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"SUMMARY BY PERIOD\")\n",
    "print(\"-\"*80)\n",
    "summary_period = df.groupby('period_label').agg({\n",
    "    'Spread': ['mean', 'median'],\n",
    "    'Volume': ['mean', 'median'],\n",
    "    'Date': 'count'\n",
    "}).round(6)\n",
    "summary_period.columns = ['Spread_Mean', 'Spread_Median', 'Volume_Mean', 'Volume_Median', 'N_Obs']\n",
    "print(summary_period)\n",
    "\n",
    "# Calculate percentage changes vs control\n",
    "control_spread = summary_period.loc['control_year', 'Spread_Mean']\n",
    "control_volume = summary_period.loc['control_year', 'Volume_Mean']\n",
    "summary_period['Spread_Change_%'] = ((summary_period['Spread_Mean'] - control_spread) / control_spread * 100).round(2)\n",
    "summary_period['Volume_Change_%'] = ((summary_period['Volume_Mean'] - control_volume) / control_volume * 100).round(2)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"CHANGES VS CONTROL PERIOD\")\n",
    "print(\"-\"*80)\n",
    "print(summary_period[['Spread_Mean', 'Spread_Change_%', 'Volume_Mean', 'Volume_Change_%']])\n",
    "\n",
    "summary_cap.to_csv('output/summary_by_cap.csv')\n",
    "summary_period.to_csv('output/summary_by_period.csv')\n",
    "\n",
    "# CRITICAL ADDITION: PRACTICAL IMPLICATIONS (NOK COST SAVINGS)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRACTICAL IMPLICATIONS: TRADING COST ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate cost savings for realistic trade sizes\n",
    "trade_sizes = [100000, 500000, 1000000, 5000000]  # NOK\n",
    "avg_stock_price = 100  # Assume 100 NOK per share\n",
    "\n",
    "spread_control = summary_period.loc['control_year', 'Spread_Mean']\n",
    "spread_summer = summary_period.loc['summer_holiday', 'Spread_Mean']\n",
    "spread_diff = spread_summer - spread_control\n",
    "\n",
    "print(f\"\\nSpread difference (Summer vs. Control): {spread_diff:.6f}\")\n",
    "print(f\"Average stock price assumed: {avg_stock_price} NOK\\n\")\n",
    "\n",
    "print(\"Cost Impact per Trade (one-way):\")\n",
    "print(f\"{'Trade Size (NOK)':>20} {'Control Cost':>15} {'Summer Cost':>15} {'Savings':>15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for trade_size in trade_sizes:\n",
    "    shares = trade_size / avg_stock_price\n",
    "    cost_control = shares * avg_stock_price * spread_control / 2  # Half-spread for one-way\n",
    "    cost_summer = shares * avg_stock_price * spread_summer / 2\n",
    "    savings = cost_control - cost_summer\n",
    "    \n",
    "    print(f\"{trade_size:>20,} {cost_control:>14.2f} NOK {cost_summer:>14.2f} NOK {savings:>14.2f} NOK\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "if spread_diff < 0:\n",
    "    print(f\"   Trading during summer holidays saves approximately {abs(spread_diff/spread_control)*100:.1f}%\")\n",
    "    print(f\"   on transaction costs due to narrower spreads.\")\n",
    "else:\n",
    "    print(f\"   Trading during summer holidays costs approximately {(spread_diff/spread_control)*100:.1f}% more\")\n",
    "    print(f\"   due to wider spreads (consistent with Harris's dealer hypothesis).\")\n",
    "\n",
    "\n",
    "# FIGURE 1: CHRISTMAS & EASTER ANALYSIS\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"GENERATING FIGURE 1: CHRISTMAS & EASTER\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "christmas_easter_periods = ['control_year', 'pre_easter', 'post_easter',\n",
    "                            'pre_christmas', 'christmas', 'post_christmas']\n",
    "df_ce = df[df['period_label'].isin(christmas_easter_periods)].copy()\n",
    "df_ce['period_label'] = pd.Categorical(df_ce['period_label'],\n",
    "                                       categories=christmas_easter_periods,\n",
    "                                       ordered=True)\n",
    "\n",
    "# Figure 1A: Spread (added confidence intervals)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.pointplot(x='period_label', y='Spread', hue='Cap_Group', data=df_ce,\n",
    "             hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "             dodge=True,\n",
    "             palette='viridis',\n",
    "             errorbar='ci',\n",
    "             capsize=0.1)\n",
    "plt.title(f'Figure 1A: Mean Spread (Christmas & Easter Periods) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Period', fontsize=14)\n",
    "plt.ylabel('Mean Spread', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_1A_CE_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_1A_CE_Spread.png\")\n",
    "\n",
    "# Figure 1B: Volume\n",
    "volume_ce = df_ce.groupby(['period_label', 'Cap_Group'], observed=False)['Volume'].mean().reset_index()\n",
    "volume_ce['Volume_millions'] = volume_ce['Volume'] / 1e6\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='period_label', y='Volume_millions', hue='Cap_Group', data=volume_ce,\n",
    "           hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "           palette='magma',\n",
    "           errorbar='ci')\n",
    "plt.title(f'Figure 1B: Mean Daily Volume (Christmas & Easter Periods) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Period', fontsize=14)\n",
    "plt.ylabel('Mean Volume (millions)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_1B_CE_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_1B_CE_Volume.png\")\n",
    "\n",
    "# FIGURE 2: SUMMER ANALYSIS\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"GENERATING FIGURE 2: SUMMER HOLIDAY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "summer_periods = ['control_year', 'summer_excl_holiday', 'summer_holiday']\n",
    "df_summer = df[df['period_label'].isin(summer_periods)].copy()\n",
    "df_summer['period_label'] = pd.Categorical(df_summer['period_label'],\n",
    "                                           categories=summer_periods,\n",
    "                                           ordered=True)\n",
    "\n",
    "# Figure 2A: Spread\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.pointplot(x='period_label', y='Spread', hue='Cap_Group', data=df_summer,\n",
    "             hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "             dodge=True,\n",
    "             palette='viridis',\n",
    "             errorbar='ci',\n",
    "             capsize=0.1)\n",
    "plt.title(f'Figure 2A: Mean Spread (Summer Holiday) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Period', fontsize=14)\n",
    "plt.ylabel('Mean Spread', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_2A_Summer_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_2A_Summer_Spread.png\")\n",
    "\n",
    "# Figure 2B: Volume\n",
    "volume_summer = df_summer.groupby(['period_label', 'Cap_Group'], observed=False)['Volume'].mean().reset_index()\n",
    "volume_summer['Volume_millions'] = volume_summer['Volume'] / 1e6\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.barplot(x='period_label', y='Volume_millions', hue='Cap_Group', data=volume_summer,\n",
    "           hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "           palette='magma',\n",
    "           errorbar='ci')\n",
    "plt.title(f'Figure 2B: Mean Daily Volume (Summer Holiday) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Period', fontsize=14)\n",
    "plt.ylabel('Mean Volume (millions)', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_2B_Summer_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_2B_Summer_Volume.png\")\n",
    "\n",
    "# FIGURE 3: P-VALUE HEATMAPS WITH MULTIPLE TESTING CORRECTION\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"GENERATING FIGURE 3: P-VALUE HEATMAPS (WITH MULTIPLE CORRECTIONS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Import multiple testing correction\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "p_values_spread = {}\n",
    "p_values_volume = {}\n",
    "\n",
    "# Collect all raw p-values for correction\n",
    "all_p_spread = []\n",
    "all_p_volume = []\n",
    "test_info = []  # To keep track of which test corresponds to which p-value\n",
    "\n",
    "for group in ['Large Cap', 'Mid Cap', 'Small Cap']:\n",
    "    p_values_spread[group] = {}\n",
    "    p_values_volume[group] = {}\n",
    "    \n",
    "    control = df_ce[(df_ce['period_label'] == 'control_year') & (df_ce['Cap_Group'] == group)]\n",
    "    \n",
    "    for period in [p for p in christmas_easter_periods if p != 'control_year']:\n",
    "        event = df_ce[(df_ce['period_label'] == period) & (df_ce['Cap_Group'] == group)]\n",
    "        \n",
    "        if len(event) > 1 and len(control) > 1:\n",
    "            _, p_spread = ttest_ind(event['Spread'].dropna(),\n",
    "                                   control['Spread'].dropna(),\n",
    "                                   equal_var=False)\n",
    "            _, p_volume = ttest_ind(event['Volume'].dropna(),\n",
    "                                   control['Volume'].dropna(),\n",
    "                                   equal_var=False)\n",
    "            \n",
    "            all_p_spread.append(p_spread)\n",
    "            all_p_volume.append(p_volume)\n",
    "            test_info.append((group, period))\n",
    "            \n",
    "            p_values_spread[group][period] = p_spread\n",
    "            p_values_volume[group][period] = p_volume\n",
    "\n",
    "# Multiple Testing Correction\n",
    "\n",
    "# Method 1: Bonferroni Correction (Conservative)\n",
    "print(\"\\n--- Applying Bonferroni Correction ---\")\n",
    "print(f'Total tests performed: {len(all_p_spread)}')\n",
    "print(f'Bonferroni-corrected alpha level: {0.05 / len(all_p_spread):.5f}\\n')\n",
    "\n",
    "reject_spread_bonf, pvals_corrected_spread_bonf, _, _ = multipletests(\n",
    "    all_p_spread, alpha=0.05, method='bonferroni'\n",
    ")\n",
    "reject_volume_bonf, pvals_corrected_volume_bonf, _, _ = multipletests(\n",
    "    all_p_volume, alpha=0.05, method='bonferroni'\n",
    ")\n",
    "\n",
    "# Method 2: Benjamini-Hochberg (FDR) Correction (Less Conservative)\n",
    "print(\"--- Applying Benjamini-Hochberg (FDR) Correction ---\")\n",
    "reject_spread_fdr, pvals_corrected_spread_fdr, _, _ = multipletests(\n",
    "    all_p_spread, alpha=0.05, method='fdr_bh'\n",
    ")\n",
    "reject_volume_fdr, pvals_corrected_volume_fdr, _, _ = multipletests(\n",
    "    all_p_volume, alpha=0.05, method='fdr_bh'\n",
    ")\n",
    "print(\"FDR correction applied.\\n\")\n",
    "\n",
    "# Create DataFrames for Heatmaps and Comparison\n",
    "\n",
    "# Create DataFrame for raw p-values (for heatmap annotation)\n",
    "p_values_spread_df = pd.DataFrame(p_values_spread).T\n",
    "p_values_volume_df = pd.DataFrame(p_values_volume).T\n",
    "\n",
    "# Create DataFrames for corrected p-values\n",
    "p_values_corrected_df_spread_bonf = pd.DataFrame(index=p_values_spread_df.index, columns=p_values_spread_df.columns)\n",
    "p_values_corrected_df_volume_bonf = pd.DataFrame(index=p_values_volume_df.index, columns=p_values_volume_df.columns)\n",
    "p_values_corrected_df_spread_fdr = pd.DataFrame(index=p_values_spread_df.index, columns=p_values_spread_df.columns)\n",
    "p_values_corrected_df_volume_fdr = pd.DataFrame(index=p_values_volume_df.index, columns=p_values_volume_df.columns)\n",
    "\n",
    "# Populate corrected p-value DataFrames\n",
    "for i, (group, period) in enumerate(test_info):\n",
    "    p_values_corrected_df_spread_bonf.loc[group, period] = pvals_corrected_spread_bonf[i]\n",
    "    p_values_corrected_df_volume_bonf.loc[group, period] = pvals_corrected_volume_bonf[i]\n",
    "    p_values_corrected_df_spread_fdr.loc[group, period] = pvals_corrected_spread_fdr[i]\n",
    "    p_values_corrected_df_volume_fdr.loc[group, period] = pvals_corrected_volume_fdr[i]\n",
    "\n",
    "# --- Print Comparison Tables ---\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"COMPARISON OF CORRECTED P-VALUES (SPREAD)\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\n--- Bonferroni Corrected P-Values (Spread) ---\")\n",
    "print(p_values_corrected_df_spread_bonf.astype(float).round(4))\n",
    "print(\"\\n--- FDR Corrected P-Values (Spread) ---\")\n",
    "print(p_values_corrected_df_spread_fdr.astype(float).round(4))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"COMPARISON OF CORRECTED P-VALUES (VOLUME)\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\n--- Bonferroni Corrected P-Values (Volume) ---\")\n",
    "print(p_values_corrected_df_volume_bonf.astype(float).round(4))\n",
    "print(\"\\n--- FDR Corrected P-Values (Volume) ---\")\n",
    "print(p_values_corrected_df_volume_fdr.astype(float).round(4))\n",
    "\n",
    "# Figure 3A: Spread p-values with Bonferroni-corrected significance\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.heatmap(p_values_spread_df, annot=True, cmap='viridis_r', fmt=\".3f\",\n",
    "            linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (uncorrected)'})\n",
    "\n",
    "# Highlight Bonferroni-significant cells (This part remains unchanged)\n",
    "for i in range(len(p_values_corrected_df_spread_bonf.index)):\n",
    "    for j in range(len(p_values_corrected_df_spread_bonf.columns)):\n",
    "        val = p_values_corrected_df_spread_bonf.iloc[i, j]\n",
    "        if val < 0.05:\n",
    "            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False,\n",
    "                                      edgecolor='red', lw=3))\n",
    "\n",
    "ax.set_title(f'Figure 3A: P-values for Spread (vs. Control) | {SPREAD_METHOD_NAME}\\nRed border = p < 0.05 (Bonferroni-corrected)',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_3A_PValues_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nSaved: Figure_3A_PValues_Spread.png\")\n",
    "\n",
    "# Figure 3B: Volume p-values with Bonferroni-corrected significance\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.heatmap(p_values_volume_df, annot=True, cmap='plasma_r', fmt=\".3f\",\n",
    "            linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (uncorrected)'})\n",
    "\n",
    "# Highlight Bonferroni-significant cells (This part remains unchanged)\n",
    "for i in range(len(p_values_corrected_df_volume_bonf.index)):\n",
    "    for j in range(len(p_values_corrected_df_volume_bonf.columns)):\n",
    "        val = p_values_corrected_df_volume_bonf.iloc[i, j]\n",
    "        if val < 0.05:\n",
    "            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False,\n",
    "                                      edgecolor='red', lw=3))\n",
    "\n",
    "ax.set_title(f'Figure 3B: P-values for Volume (vs. Control) | {SPREAD_METHOD_NAME}\\nRed border = p < 0.05 (Bonferroni-corrected)',\n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_3B_PValues_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_3B_PValues_Volume.png\")\n",
    "\n",
    "# FIGURE 4: HEATMAP - SPREAD BY CAP AND PERIOD\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"GENERATING FIGURE 4: HEATMAP\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "period_order = ['control_year', 'pre_easter', 'post_easter',\n",
    "                'pre_christmas', 'christmas', 'post_christmas',\n",
    "                'summer_excl_holiday', 'summer_holiday']\n",
    "period_order = [p for p in period_order if p in df['period_label'].unique()]\n",
    "\n",
    "heatmap_data = df.groupby(['Cap_Group', 'period_label'])['Spread'].mean().unstack()\n",
    "heatmap_data = heatmap_data[period_order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "sns.heatmap(heatmap_data,\n",
    "            annot=True,\n",
    "            fmt='.5f',\n",
    "            cmap='YlOrRd',\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Avg Spread'},\n",
    "            ax=ax,\n",
    "            annot_kws={'fontsize': 11})\n",
    "\n",
    "ax.set_title(f'Figure 4: Average Spread by Cap Group and Period | {SPREAD_METHOD_NAME}',\n",
    "             fontweight='bold',\n",
    "             fontsize=18,\n",
    "             pad=15)\n",
    "\n",
    "ax.set_xlabel('Period', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Cap Group', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=13)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Figure_4_Heatmap_Spread.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_4_Heatmap_Spread.png\")\n",
    "\n",
    "# FIGURE 5: TIME SERIES - MONTHLY AVERAGE SPREAD\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"GENERATING FIGURE 5: TIME SERIES | {SPREAD_METHOD_NAME}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "monthly_spread = df.groupby('YearMonth')['Spread'].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(monthly_spread.index.to_timestamp(), monthly_spread.values,\n",
    "        color='darkblue', linewidth=1.5, alpha=0.8)\n",
    "ax.set_title(f'Figure 5: Monthly Average Spread (2014-2024) | {SPREAD_METHOD_NAME}', fontweight='bold', fontsize=16)\n",
    "ax.set_xlabel('Date', fontsize=14)\n",
    "ax.set_ylabel('Average Spread', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "covid_start = pd.Timestamp('2020-03-01')\n",
    "covid_end = pd.Timestamp('2020-12-31')\n",
    "ax.axvspan(covid_start, covid_end, alpha=0.2, color='red', label='COVID-19')\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Figure_5_Timeseries_Monthly.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved: Figure_5_Timeseries_Monthly.png\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"COMPLETE - 5 essential figures created\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\nAll outputs saved to 'output/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945f6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SCRIPT 3: OLS REGRESSION ANALYSIS\n",
      "Using Dependent Variable: Corwin-Schultz (2012) Spread\n",
      "--------------------------------------------------------------------------------\n",
      "Successfully loaded data from oslo_bors_labelled_data.csv\n",
      "\n",
      "--- Preparing data for regression analysis ---\n",
      "Regression sample created with 118,148 observations.\n",
      "\n",
      "--- VIF MULTICOLLINEARITY DIAGNOSTICS ---\n",
      "Variance Inflation Factors:\n",
      "       Variable      VIF\n",
      "Log_Volume_lag1 4.872278\n",
      "      Large_Cap 2.550172\n",
      "        Mid_Cap 2.003723\n",
      "     Volatility 1.851746\n",
      "         Summer 1.066546\n",
      "      Christmas 1.046437\n",
      "  Pre_Christmas 1.024273\n",
      "     Pre_Easter 1.023075\n",
      "    Post_Easter 1.021600\n",
      " Post_Christmas 1.020296\n",
      "\n",
      "--- MODELS 1A-1C: BASELINE HOLIDAY EFFECTS BY MARKET CAP ---\n",
      "\n",
      "Processing Model 1A: Large Cap\n",
      "SUCCESS: Saved model summary to output/Table_1A_Baseline_Large_Cap.png\n",
      "\n",
      "Processing Model 1B: Mid Cap\n",
      "SUCCESS: Saved model summary to output/Table_1B_Baseline_Mid_Cap.png\n",
      "\n",
      "Processing Model 1C: Small Cap\n",
      "SUCCESS: Saved model summary to output/Table_1C_Baseline_Small_Cap.png\n",
      "\n",
      "--- MODEL 2: SPREAD WITH CONTROL VARIABLES ---\n",
      "SUCCESS: Saved model summary to output/Table_2_Spread_Full_Controls.png\n",
      "\n",
      "--- MODEL 3: OLS WITH STOCK FIXED EFFECTS ---\n",
      "Model 3: R-squared = 0.2542, N = 118148\n",
      "\n",
      "Key Coefficients:\n",
      "                 Coefficient  Std Error     P>|t|\n",
      "const              -0.010153   0.002962  0.000608\n",
      "Pre_Easter         -0.000047   0.000338  0.889710\n",
      "Post_Easter        -0.000466   0.000328  0.156061\n",
      "Christmas           0.000212   0.000297  0.475154\n",
      "Pre_Christmas      -0.000186   0.000413  0.652483\n",
      "Post_Christmas      0.000625   0.000344  0.069161\n",
      "Summer             -0.001333   0.000275  0.000001\n",
      "Log_Volume_lag1     0.001568   0.000257  0.000000\n",
      "Volatility          0.075453   0.020382  0.000214\n",
      "SUCCESS: Saved model summary to output/Table_3_Stock_Fixed_Effects.png\n",
      "\n",
      "--- MODEL 4: PANEL REGRESSION WITH ENTITY & TIME FIXED EFFECTS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/whd5k2z14l1f9v052bj7zhfm0000gn/T/ipykernel_16338/2679274653.py:165: AbsorbingEffectWarning: \n",
      "Variables have been fully absorbed and have removed from the regression:\n",
      "\n",
      "Pre_Easter, Post_Easter, Christmas, Pre_Christmas, Post_Christmas, Summer\n",
      "\n",
      "  model4 = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=True, drop_absorbed=True).fit(cov_type='clustered', cluster_entity=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4: R-squared (within) = 0.0442, N = 118148\n",
      "\n",
      "Panel Regression Coefficients:\n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "Log_Volume_lag1     0.0014     0.0003     5.6701     0.0000      0.0009      0.0020\n",
      "Volatility          0.0682     0.0181     3.7601     0.0002      0.0327      0.1038\n",
      "===================================================================================\n",
      "SUCCESS: Saved model summary to output/Table_4_Panel_Entity_Time_FE.png\n",
      "\n",
      "--- MODEL COMPARISON ---\n",
      "             Model  Summer Coef  Summer P-val  R-squared\n",
      "     Model 2 (OLS)    -0.001510  4.877897e-07   0.138996\n",
      "Model 3 (Stock FE)    -0.001333  1.314628e-06   0.254227\n",
      "Model 4 (Panel FE)          NaN           NaN   0.044217\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Script finished. All regression tables have been saved as PNG images in the 'output' folder.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 3: OLS REGRESSION ANALYSIS WITH DYNAMIC DESCRIPTIVE TITLES\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"SCRIPT 3: OLS REGRESSION ANALYSIS\")\n",
    "print(f\"Using Dependent Variable: {SPREAD_METHOD_NAME}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, parse_dates=['Date'])\n",
    "    print(f\"Successfully loaded data from {INPUT_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_CSV}' was not found. Please ensure SCRIPT 1 has been run successfully.\")\n",
    "    exit()\n",
    "\n",
    "# Data Preparation\n",
    "print(\"\\n--- Preparing data for regression analysis ---\")\n",
    "df = df.sort_values(['Ticker', 'Date'])\n",
    "df['Return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "df['Volatility'] = df.groupby('Ticker')['Return'].transform(lambda x: x.shift(1).rolling(window=20, min_periods=10).std())\n",
    "df['Log_Volume'] = np.log1p(df['Volume'])\n",
    "df['Log_Volume_lag1'] = df.groupby('Ticker')['Log_Volume'].shift(1)\n",
    "\n",
    "# Holiday and Market Cap Dummies\n",
    "df['Pre_Easter'] = (df['period_label'] == 'pre_easter').astype(int)\n",
    "df['Post_Easter'] = (df['period_label'] == 'post_easter').astype(int)\n",
    "df['Christmas'] = (df['period_label'] == 'christmas').astype(int)\n",
    "df['Pre_Christmas'] = (df['period_label'] == 'pre_christmas').astype(int)\n",
    "df['Post_Christmas'] = (df['period_label'] == 'post_christmas').astype(int)\n",
    "df['Summer'] = (df['period_label'] == 'summer_holiday').astype(int)\n",
    "df['Large_Cap'] = (df['Cap_Group'] == 'Large Cap').astype(int)\n",
    "df['Mid_Cap'] = (df['Cap_Group'] == 'Mid Cap').astype(int)\n",
    "\n",
    "# Create regression sample\n",
    "df_reg = df.dropna(subset=['Spread', 'Log_Volume_lag1', 'Volatility']).copy()\n",
    "print(f\"Regression sample created with {len(df_reg):,} observations.\")\n",
    "\n",
    "# VIF MULTICOLLINEARITY CHECK\n",
    "print(\"\\n--- VIF MULTICOLLINEARITY DIAGNOSTICS ---\")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_vars = ['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', \n",
    "            'Summer', 'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']\n",
    "X_vif = df_reg[vif_vars].copy()\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(\"\\nWARNING: High multicollinearity detected (VIF > 10)\")\n",
    "    print(high_vif.to_string(index=False))\n",
    "\n",
    "# Models 1A-C: Baseline Holiday Effects by Market Cap\n",
    "print(\"\\n--- MODELS 1A-1C: BASELINE HOLIDAY EFFECTS BY MARKET CAP ---\")\n",
    "baseline_models = {}\n",
    "for i, cap in enumerate(['Large Cap', 'Mid Cap', 'Small Cap']):\n",
    "    print(f\"\\nProcessing Model 1{chr(65+i)}: {cap}\")\n",
    "    df_cap = df_reg[df_reg['Cap_Group'] == cap].copy()\n",
    "    \n",
    "    y = df_cap['Spread']\n",
    "    X = df_cap[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer']]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': df_cap['Ticker']})\n",
    "    baseline_models[cap] = model\n",
    "    \n",
    "    # Create and save summary as an image with the dynamic title\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.axis('off')\n",
    "    title_text = f\"Table 1{chr(65+i)}: Baseline Spread Regression - {cap} Stocks\\nDependent Variable: {SPREAD_METHOD_NAME}\"\n",
    "    ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "    summary_text = str(model.summary())\n",
    "    ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "    \n",
    "    filename = f\"output/Table_1{chr(65+i)}_Baseline_{cap.replace(' ', '_')}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"SUCCESS: Saved model summary to {filename}\")\n",
    "\n",
    "# Model 2: Full Sample with Controls\n",
    "print(\"\\n--- MODEL 2: SPREAD WITH CONTROL VARIABLES ---\")\n",
    "X2 = df_reg[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer',\n",
    "             'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']]\n",
    "X2 = sm.add_constant(X2)\n",
    "y2 = df_reg['Spread']\n",
    "model2 = sm.OLS(y2, X2).fit(cov_type='cluster', cov_kwds={'groups': df_reg['Ticker']})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = (f'Table 2: Spread Determinants with Control Variables (Full Sample)\\n'\n",
    "              f'Dependent Variable: {SPREAD_METHOD_NAME} | Controls: Lagged Volume, Volatility, Market Cap')\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model2.summary())\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_2 = 'output/Table_2_Spread_Full_Controls.png'\n",
    "plt.savefig(filename_2, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved model summary to {filename_2}\")\n",
    "\n",
    "# MODEL 3: STOCK FIXED EFFECTS\n",
    "print(\"\\n--- MODEL 3: OLS WITH STOCK FIXED EFFECTS ---\")\n",
    "stock_dummies = pd.get_dummies(df_reg['Ticker'], prefix='Stock', drop_first=True, dtype=float)\n",
    "\n",
    "X3 = pd.concat([\n",
    "    df_reg[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer',\n",
    "            'Log_Volume_lag1', 'Volatility']].astype(float),\n",
    "    stock_dummies\n",
    "], axis=1)\n",
    "X3 = sm.add_constant(X3)\n",
    "\n",
    "model3 = sm.OLS(df_reg['Spread'].astype(float), X3).fit(cov_type='cluster', cov_kwds={'groups': df_reg['Ticker']})\n",
    "\n",
    "print(f\"Model 3: R-squared = {model3.rsquared:.4f}, N = {model3.nobs:.0f}\")\n",
    "\n",
    "coef_interest = ['const', 'Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', \n",
    "                 'Post_Christmas', 'Summer', 'Log_Volume_lag1', 'Volatility']\n",
    "coef_table = pd.DataFrame({\n",
    "    'Coefficient': model3.params[coef_interest],\n",
    "    'Std Error': model3.bse[coef_interest],\n",
    "    'P>|t|': model3.pvalues[coef_interest]\n",
    "}).round(6)\n",
    "\n",
    "print(\"\\nKey Coefficients:\")\n",
    "print(coef_table.to_string())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.axis('off')\n",
    "title_text = f'Table 3: Spread Regression with Stock Fixed Effects\\nDependent Variable: {SPREAD_METHOD_NAME}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = f\"\"\"OLS Regression Results (with Stock FE)\n",
    "R-squared: {model3.rsquared:.4f}\n",
    "Observations: {model3.nobs:.0f}\n",
    "Stock FE: {len(stock_dummies.columns)} dummies included\n",
    "\n",
    "{coef_table.to_string()}\n",
    "\n",
    "Notes: Standard errors clustered by Ticker\n",
    "\"\"\"\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_3 = 'output/Table_3_Stock_Fixed_Effects.png'\n",
    "plt.savefig(filename_3, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved model summary to {filename_3}\")\n",
    "\n",
    "# MODEL 4: PANEL REGRESSION\n",
    "print(\"\\n--- MODEL 4: PANEL REGRESSION WITH ENTITY & TIME FIXED EFFECTS ---\")\n",
    "from linearmodels.panel import PanelOLS\n",
    "\n",
    "df_panel = df_reg.set_index(['Ticker', 'Date']).copy()\n",
    "y_panel = df_panel['Spread']\n",
    "X_panel = df_panel[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', \n",
    "                    'Post_Christmas', 'Summer', 'Log_Volume_lag1', 'Volatility']]\n",
    "\n",
    "model4 = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=True, drop_absorbed=True).fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "print(f\"Model 4: R-squared (within) = {model4.rsquared:.4f}, N = {model4.nobs:.0f}\")\n",
    "print(\"\\nPanel Regression Coefficients:\")\n",
    "print(model4.summary.tables[1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = f'Table 4: Panel Regression with Entity & Time Fixed Effects\\nDependent Variable: {SPREAD_METHOD_NAME}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model4.summary)\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 8}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_4 = 'output/Table_4_Panel_Entity_Time_FE.png'\n",
    "plt.savefig(filename_4, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved model summary to {filename_4}\")\n",
    "\n",
    "# MODEL COMPARISON\n",
    "print(\"\\n--- MODEL COMPARISON ---\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Model 2 (OLS)', 'Model 3 (Stock FE)', 'Model 4 (Panel FE)'],\n",
    "    'Summer Coef': [model2.params.get('Summer', np.nan), model3.params.get('Summer', np.nan), model4.params.get('Summer', np.nan)],\n",
    "    'Summer P-val': [model2.pvalues.get('Summer', np.nan), model3.pvalues.get('Summer', np.nan), model4.pvalues.get('Summer', np.nan)],\n",
    "    'R-squared': [model2.rsquared, model3.rsquared, model4.rsquared]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "comparison_df.to_csv('output/Model_Comparison_Summary.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Script finished. All regression tables have been saved as PNG images in the 'output' folder.\")\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cef62776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SCRIPT 4: ROBUSTNESS CHECK - EXCLUDING COVID PERIOD\n",
      "--------------------------------------------------------------------------------\n",
      "Successfully loaded data from 'oslo_bors_labelled_data.csv'\n",
      "\n",
      "--- Excluding the entire year 2020 for robustness check ---\n",
      "Original sample size: 119,138 observations.\n",
      "Robust sample size (2020 excluded): 107,767 observations.\n",
      "Removed 11,371 observations.\n",
      "\n",
      "--- Re-creating analysis variables for the robust sample ---\n",
      "Removed 990 rows with missing values needed for regression.\n",
      "Final robust sample size: 106,777 observations.\n",
      "\n",
      "--- VIF MULTICOLLINEARITY DIAGNOSTICS (ROBUST SAMPLE) ---\n",
      "Variance Inflation Factors:\n",
      "       Variable      VIF\n",
      "Log_Volume_lag1 4.093354\n",
      "      Large_Cap 2.521512\n",
      "        Mid_Cap 1.992370\n",
      "     Volatility 1.339228\n",
      "         Summer 1.061121\n",
      "      Christmas 1.041688\n",
      "\n",
      "--- MODEL 1 (ROBUST): BASELINE OLS ---\n",
      "Model 1 (Robust): R-squared = 0.0839, N = 106777\n",
      "SUCCESS: Saved to 'output/Table_5A_Robustness_No_COVID.png'\n",
      "\n",
      "--- MODEL 2 (ROBUST): OLS WITH STOCK FIXED EFFECTS ---\n",
      "Model 2 (Robust FE): R-squared = 0.2513, N = 106777\n",
      "\n",
      "Key Coefficients:\n",
      "                 Coefficient  Std Error     P>|t|\n",
      "const              -0.009098   0.003045  0.002810\n",
      "Summer             -0.001208   0.000283  0.000019\n",
      "Christmas           0.000764   0.000321  0.017501\n",
      "Log_Volume_lag1     0.001539   0.000251  0.000000\n",
      "Volatility          0.023461   0.012907  0.069109\n",
      "SUCCESS: Saved to 'output/Table_5B_Robustness_Stock_FE.png'\n",
      "\n",
      "--- MODEL 3 (ROBUST): PANEL REGRESSION WITH ENTITY & TIME FIXED EFFECTS ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/whd5k2z14l1f9v052bj7zhfm0000gn/T/ipykernel_16338/2802494956.py:143: AbsorbingEffectWarning: \n",
      "Variables have been fully absorbed and have removed from the regression:\n",
      "\n",
      "Summer, Christmas\n",
      "\n",
      "  model_robust_panel = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=True, drop_absorbed=True).fit(cov_type='clustered', cluster_entity=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 (Robust Panel): R-squared (within) = 0.0284, N = 106777\n",
      "\n",
      "Panel Regression Coefficients:\n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "Log_Volume_lag1     0.0015     0.0003     6.0546     0.0000      0.0010      0.0020\n",
      "Volatility          0.0232     0.0126     1.8353     0.0665     -0.0016      0.0479\n",
      "===================================================================================\n",
      "SUCCESS: Saved to 'output/Table_5C_Robustness_Panel_FE.png'\n",
      "\n",
      "--- ROBUSTNESS MODEL COMPARISON ---\n",
      "\n",
      "Robustness Check - Holiday Effects Across Specifications:\n",
      "          Model  Summer Coef  Summer P-val  Christmas Coef  Christmas P-val  R-squared\n",
      "     Robust OLS    -0.001388      0.000004        0.001015         0.002892   0.083928\n",
      "Robust Stock FE    -0.001208      0.000019        0.000764         0.017501   0.251285\n",
      "Robust Panel FE          NaN           NaN             NaN              NaN   0.028392\n",
      "\n",
      "SUCCESS: Saved comparison to 'output/Robustness_Model_Comparison.csv'\n",
      "\n",
      "--- Creating coefficient summary table ---\n",
      "\n",
      "Key Coefficients (Robust Sample - Model 1):\n",
      " Variable  Coefficient  Std_Error      P_value Significance\n",
      "   Summer    -0.001388   0.000303 4.492754e-06          ***\n",
      "Christmas     0.001015   0.000341 2.892478e-03          ***\n",
      "Large_Cap    -0.008374   0.001564 8.608005e-08          ***\n",
      "  Mid_Cap    -0.003554   0.002045 8.220664e-02            *\n",
      "\n",
      "SUCCESS: Coefficient summary saved to 'output/Robustness_Coefficients_Summary.csv'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ROBUSTNESS CHECK SCRIPT COMPLETE\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 4: ROBUSTNESS CHECK - EXCLUDE COVID PERIOD\n",
    "print(\"-\" * 80)\n",
    "print(\"SCRIPT 4: ROBUSTNESS CHECK - EXCLUDING COVID PERIOD\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Define the input file and output directory\n",
    "INPUT_CSV = \"oslo_bors_labelled_data.csv\"\n",
    "output_dir = 'output'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV, parse_dates=['Date'])\n",
    "    print(f\"Successfully loaded data from '{INPUT_CSV}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_CSV}' was not found. Please ensure it's in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# Exclude the COVID-19 Period\n",
    "print(\"\\n--- Excluding the entire year 2020 for robustness check ---\")\n",
    "df_robust = df[df['Date'].dt.year != 2020].copy()\n",
    "print(f\"Original sample size: {len(df):,} observations.\")\n",
    "print(f\"Robust sample size (2020 excluded): {len(df_robust):,} observations.\")\n",
    "print(f\"Removed {len(df) - len(df_robust):,} observations.\")\n",
    "\n",
    "# Re-create all necessary variables\n",
    "print(\"\\n--- Re-creating analysis variables for the robust sample ---\")\n",
    "df_robust = df_robust.sort_values(['Ticker', 'Date'])\n",
    "df_robust['Return'] = df_robust.groupby('Ticker')['Close'].pct_change()\n",
    "df_robust['Volatility'] = df_robust.groupby('Ticker')['Return'].transform(lambda x: x.shift(1).rolling(window=20, min_periods=10).std())\n",
    "df_robust['Log_Volume'] = np.log1p(df_robust['Volume'])\n",
    "df_robust['Log_Volume_lag1'] = df_robust.groupby('Ticker')['Log_Volume'].shift(1)\n",
    "df_robust['Large_Cap'] = (df_robust['Cap_Group'] == 'Large Cap').astype(int)\n",
    "df_robust['Mid_Cap'] = (df_robust['Cap_Group'] == 'Mid Cap').astype(int)\n",
    "df_robust['Summer'] = (df_robust['period_label'] == 'summer_holiday').astype(int)\n",
    "df_robust['Christmas'] = (df_robust['period_label'] == 'christmas').astype(int)\n",
    "\n",
    "# Drop rows with missing values\n",
    "initial_len = len(df_robust)\n",
    "df_robust = df_robust.dropna(subset=['Spread', 'Log_Volume_lag1', 'Volatility']).copy()\n",
    "print(f\"Removed {initial_len - len(df_robust):,} rows with missing values needed for regression.\")\n",
    "print(f\"Final robust sample size: {len(df_robust):,} observations.\")\n",
    "\n",
    "# VIF MULTICOLLINEARITY CHECK\n",
    "print(\"\\n--- VIF MULTICOLLINEARITY DIAGNOSTICS (ROBUST SAMPLE) ---\")\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_vars = ['Summer', 'Christmas', 'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']\n",
    "X_vif = df_robust[vif_vars].copy()\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(\"\\nWARNING: High multicollinearity detected (VIF > 10)\")\n",
    "    print(high_vif.to_string(index=False))\n",
    "\n",
    "# MODEL 1: BASELINE OLS\n",
    "print(\"\\n--- MODEL 1 (ROBUST): BASELINE OLS ---\")\n",
    "y = df_robust['Spread']\n",
    "X = df_robust[['Summer', 'Christmas', 'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model_robust = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': df_robust['Ticker']})\n",
    "\n",
    "print(f\"Model 1 (Robust): R-squared = {model_robust.rsquared:.4f}, N = {model_robust.nobs:.0f}\")\n",
    "\n",
    "# Save Model 1\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = ('Table 5A: Robustness Check - Excluding COVID-19 Period (2020)\\n'\n",
    "              f'Dependent Variable: {SPREAD_METHOD_NAME} | Controls: Lagged Volume, Volatility, Market Cap')\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model_robust.summary())\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "output_filename_png = 'output/Table_5A_Robustness_No_COVID.png'\n",
    "plt.savefig(output_filename_png, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved to '{output_filename_png}'\")\n",
    "\n",
    "# MODEL 2: STOCK FIXED EFFECTS\n",
    "print(\"\\n--- MODEL 2 (ROBUST): OLS WITH STOCK FIXED EFFECTS ---\")\n",
    "stock_dummies = pd.get_dummies(df_robust['Ticker'], prefix='Stock', drop_first=True, dtype=float)\n",
    "\n",
    "X2 = pd.concat([\n",
    "    df_robust[['Summer', 'Christmas', 'Log_Volume_lag1', 'Volatility']].astype(float),\n",
    "    stock_dummies\n",
    "], axis=1)\n",
    "X2 = sm.add_constant(X2)\n",
    "\n",
    "model_robust_fe = sm.OLS(df_robust['Spread'].astype(float), X2).fit(cov_type='cluster', cov_kwds={'groups': df_robust['Ticker']})\n",
    "\n",
    "print(f\"Model 2 (Robust FE): R-squared = {model_robust_fe.rsquared:.4f}, N = {model_robust_fe.nobs:.0f}\")\n",
    "\n",
    "coef_interest = ['const', 'Summer', 'Christmas', 'Log_Volume_lag1', 'Volatility']\n",
    "coef_table = pd.DataFrame({\n",
    "    'Coefficient': model_robust_fe.params[coef_interest],\n",
    "    'Std Error': model_robust_fe.bse[coef_interest],\n",
    "    'P>|t|': model_robust_fe.pvalues[coef_interest]\n",
    "}).round(6)\n",
    "\n",
    "print(\"\\nKey Coefficients:\")\n",
    "print(coef_table.to_string())\n",
    "\n",
    "# Save Model 2\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.axis('off')\n",
    "title_text = f'Table 5B: Robustness with Stock Fixed Effects (No COVID)\\nDependent Variable: {SPREAD_METHOD_NAME}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = f\"\"\"OLS Regression Results (with Stock FE, No COVID)\n",
    "R-squared: {model_robust_fe.rsquared:.4f}\n",
    "Observations: {model_robust_fe.nobs:.0f}\n",
    "Stock FE: {len(stock_dummies.columns)} dummies included\n",
    "\n",
    "{coef_table.to_string()}\n",
    "\n",
    "Notes: Standard errors clustered by Ticker\n",
    "\"\"\"\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_2 = 'output/Table_5B_Robustness_Stock_FE.png'\n",
    "plt.savefig(filename_2, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved to '{filename_2}'\")\n",
    "\n",
    "# MODEL 3: PANEL REGRESSION\n",
    "print(\"\\n--- MODEL 3 (ROBUST): PANEL REGRESSION WITH ENTITY & TIME FIXED EFFECTS ---\")\n",
    "from linearmodels.panel import PanelOLS\n",
    "\n",
    "df_panel = df_robust.set_index(['Ticker', 'Date']).copy()\n",
    "y_panel = df_panel['Spread']\n",
    "X_panel = df_panel[['Summer', 'Christmas', 'Log_Volume_lag1', 'Volatility']]\n",
    "\n",
    "model_robust_panel = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=True, drop_absorbed=True).fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "print(f\"Model 3 (Robust Panel): R-squared (within) = {model_robust_panel.rsquared:.4f}, N = {model_robust_panel.nobs:.0f}\")\n",
    "print(\"\\nPanel Regression Coefficients:\")\n",
    "print(model_robust_panel.summary.tables[1])\n",
    "\n",
    "# Save Model 3\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = f'Table 5C: Robustness Panel Regression (No COVID)\\nDependent Variable: {SPREAD_METHOD_NAME}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model_robust_panel.summary)\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 8}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_3 = 'output/Table_5C_Robustness_Panel_FE.png'\n",
    "plt.savefig(filename_3, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved to '{filename_3}'\")\n",
    "\n",
    "# MODEL COMPARISON\n",
    "print(\"\\n--- ROBUSTNESS MODEL COMPARISON ---\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Robust OLS', 'Robust Stock FE', 'Robust Panel FE'],\n",
    "    'Summer Coef': [model_robust.params.get('Summer', np.nan), model_robust_fe.params.get('Summer', np.nan), model_robust_panel.params.get('Summer', np.nan)],\n",
    "    'Summer P-val': [model_robust.pvalues.get('Summer', np.nan), model_robust_fe.pvalues.get('Summer', np.nan), model_robust_panel.pvalues.get('Summer', np.nan)],\n",
    "    'Christmas Coef': [model_robust.params.get('Christmas', np.nan), model_robust_fe.params.get('Christmas', np.nan), model_robust_panel.params.get('Christmas', np.nan)],\n",
    "    'Christmas P-val': [model_robust.pvalues.get('Christmas', np.nan), model_robust_fe.pvalues.get('Christmas', np.nan), model_robust_panel.pvalues.get('Christmas', np.nan)],\n",
    "    'R-squared': [model_robust.rsquared, model_robust_fe.rsquared, model_robust_panel.rsquared]\n",
    "})\n",
    "print(\"\\nRobustness Check - Holiday Effects Across Specifications:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "comparison_df.to_csv('output/Robustness_Model_Comparison.csv', index=False)\n",
    "print(\"\\nSUCCESS: Saved comparison to 'output/Robustness_Model_Comparison.csv'\")\n",
    "\n",
    "# Coefficient Summary\n",
    "print(\"\\n--- Creating coefficient summary table ---\")\n",
    "coef_summary = pd.DataFrame({\n",
    "    'Variable': ['Summer', 'Christmas', 'Large_Cap', 'Mid_Cap'],\n",
    "    'Coefficient': model_robust.params[['Summer', 'Christmas', 'Large_Cap', 'Mid_Cap']],\n",
    "    'Std_Error': model_robust.bse[['Summer', 'Christmas', 'Large_Cap', 'Mid_Cap']],\n",
    "    'P_value': model_robust.pvalues[['Summer', 'Christmas', 'Large_Cap', 'Mid_Cap']]\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "coef_summary['Significance'] = coef_summary['P_value'].apply(lambda x: '***' if x < 0.01 else ('**' if x < 0.05 else ('*' if x < 0.1 else '')))\n",
    "print(\"\\nKey Coefficients (Robust Sample - Model 1):\")\n",
    "print(coef_summary.to_string(index=False))\n",
    "\n",
    "output_filename_csv = 'output/Robustness_Coefficients_Summary.csv'\n",
    "coef_summary.to_csv(output_filename_csv, index=False)\n",
    "print(f\"\\nSUCCESS: Coefficient summary saved to '{output_filename_csv}'\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ROBUSTNESS CHECK SCRIPT COMPLETE\")\n",
    "print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
