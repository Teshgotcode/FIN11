{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44071e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 0: INSTALL & IMPORT LIBRARIES\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "required_packages = [\n",
    "    \"yfinance\", \"pandas\", \"numpy\", \"holidays\", \"matplotlib\", \n",
    "    \"seaborn\", \"scipy\", \"statsmodels\", \"linearmodels\", \"stargazer\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from linearmodels.panel import PanelOLS\n",
    "from stargazer.stargazer import Stargazer\n",
    "import os\n",
    "\n",
    "disable_warnings = True\n",
    "if disable_warnings:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993d9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1: SETUP - DEFINE CONSTANTS AND TICKERS\n",
    "\n",
    "TICKERS = {\n",
    "    \"Large Cap\": [\n",
    "        'EQNR.OL', 'DNB.OL', 'KOG.OL', 'MOWI.OL', 'TEL.OL', 'NHY.OL', 'AKRBP.OL',\n",
    "        'ORK.OL', 'STB.OL', 'YAR.OL', 'SUBC.OL', 'GJF.OL', 'SALM.OL', 'TGS.OL',\n",
    "        'TOM.OL', 'VAR.OL', 'NEL.OL', 'FRO.OL', 'BWLPG.OL', 'HAUTO.OL',\n",
    "        'NOD.OL', 'WAWI.OL', 'NAS.OL', 'BAKKA.OL', 'WWI.OL', 'AFK.OL',\n",
    "        'AUSS.OL', 'SCATC.OL', 'MPCC.OL', 'HAFNI.OL'\n",
    "    ],\n",
    "    \"Mid Cap\": [\n",
    "        'AKER.OL', 'LSG.OL', 'KIT.OL', 'AKSO.OL', 'PARB.OL', 'BONHR.OL',\n",
    "        'BOUV.OL', 'DNO.OL', 'ENTRA.OL', 'FLNG.OL', 'MING.OL', 'NAPA.OL',\n",
    "        'NORBT.OL', 'OLT.OL', 'PCIB.OL', 'REACH.OL', 'WSTEP.OL', 'KOA.OL',\n",
    "        'HSPG.OL', 'SOFF.OL', 'ABG.OL', 'BGBIO.OL', 'EMGS.OL', 'EXTX.OL',\n",
    "        'HAVI.OL', 'HELG.OL', 'IDEX.OL', 'JIN.OL', 'MULTI.OL', 'NYKD.OL'\n",
    "    ],\n",
    "    \"Small Cap\": [\n",
    "        'QEC.OL', 'RECSI.OL', 'SPOL.OL', 'AZT.OL', 'KID.OL', 'SATS.OL',\n",
    "        'AURG.OL', 'PEN.OL', 'LINK.OL', 'PROT.OL', 'IOX.OL', 'ACC.OL',\n",
    "        'TECH.OL', 'CONTX.OL', 'NONG.OL', 'BEWI.OL', 'ELO.OL', 'GSF.OL',\n",
    "        'PRS.OL', 'AIRX.OL', 'OBSRV.OL', 'HUNT.OL', 'AKVA.OL', 'HEX.OL',\n",
    "        'SOFTX.OL', 'ASA.OL', 'NORTH.OL', 'CAPSL.OL', 'LYTIX.OL', 'VOW.OL'\n",
    "    ]\n",
    "}\n",
    "\n",
    "START_DATE = \"2014-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "OUTPUT_CSV = \"oslo_bors_labelled_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1e9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1.1: CORWIN-SCHULTZ, ABDI-RANALDO, AND ROLL SPREAD ESTIMATORS\n",
    "def corwin_schultz_spread(group):\n",
    "    \"\"\"\n",
    "    Corwin-Schultz (2012) High-Low spread estimator.\n",
    "    This function calculates a daily spread estimate.\n",
    "    \n",
    "    Reference: Corwin, S. A., & Schultz, P. (2012). A simple way to estimate \n",
    "    bid-ask spreads from daily high and low prices. The Journal of Finance, 67(2), 719-760.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "    \n",
    "    high = group['High'].values\n",
    "    low = group['Low'].values\n",
    "    \n",
    "    spreads = []\n",
    "    negative_count = 0\n",
    "    \n",
    "    for i in range(len(group)):\n",
    "        if i == 0:\n",
    "            spreads.append(np.nan)\n",
    "        else:\n",
    "            h0, l0 = high[i], low[i]\n",
    "            h1, l1 = high[i-1], low[i-1]\n",
    "            \n",
    "            if h0 > 0 and l0 > 0 and h1 > 0 and l1 > 0:\n",
    "                beta = (np.log(h0/l0))**2 + (np.log(h1/l1))**2\n",
    "                h_max = max(h0, h1)\n",
    "                l_min = min(l0, l1)\n",
    "                gamma = (np.log(h_max/l_min))**2\n",
    "                \n",
    "                k = (3 - 2*np.sqrt(2))\n",
    "                alpha = (np.sqrt(2*beta) - np.sqrt(beta)) / k - np.sqrt(gamma / k)\n",
    "                \n",
    "                spread = 2 * (np.exp(alpha) - 1) / (1 + np.exp(alpha))\n",
    "                \n",
    "                if spread >= 0:\n",
    "                    spreads.append(spread)\n",
    "                else:\n",
    "                    spreads.append(np.nan)\n",
    "                    negative_count += 1\n",
    "            else:\n",
    "                spreads.append(np.nan)\n",
    "    \n",
    "    if negative_count > 0:\n",
    "        ticker = group['Ticker'].iloc[0] if 'Ticker' in group.columns else 'Unknown'\n",
    "        # print(f\"  Warning: {ticker} had {negative_count} negative Corwin-Schultz estimates (set to NaN)\")\n",
    "    \n",
    "    group['Spread'] = spreads\n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "def abdi_ranaldo_spread(group):\n",
    "    \"\"\"\n",
    "    Calculates the NORMALIZED Abdi and Ranaldo (2017) \"BAR\" spread estimator.\n",
    "    This version converts the absolute spread (in currency) to a relative \n",
    "    spread (as a percentage of the price), making it comparable across stocks.\n",
    "    \n",
    "    Note: Additional filtering is applied to remove economically meaningless spreads.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "\n",
    "    delta_p_t = group['Close'].diff()\n",
    "    delta_p_t_minus_1 = delta_p_t.shift(1)\n",
    "\n",
    "    if delta_p_t.count() < 2:\n",
    "        return pd.Series(np.nan, index=group.index, name='Spread')\n",
    "\n",
    "    cov_term = (delta_p_t * delta_p_t_minus_1)\n",
    "    \n",
    "    # Calculate the absolute spread (in currency units)\n",
    "    absolute_spreads = 2 * np.sqrt(np.maximum(0, -cov_term))\n",
    "    \n",
    "    # NORMALIZATION STEP\n",
    "    # To get a relative spread, divide by the average of today's and yesterday's close price.\n",
    "    # Rolling window to get the average price at each point in time.\n",
    "    avg_price = group['Close'].rolling(window=2, min_periods=1).mean()\n",
    "    \n",
    "    # Calculate the relative spread. Use .values to avoid index alignment issues.\n",
    "    relative_spreads = absolute_spreads / avg_price.values\n",
    "    \n",
    "    # CRITICAL FIX: Filter out economically meaningless spreads\n",
    "    # Spreads below 0.01% (0.0001) are likely measurement errors or rounding artifacts\n",
    "    relative_spreads = np.where(relative_spreads < 0.0001, np.nan, relative_spreads)\n",
    "    \n",
    "    # Assign the comparable, relative spreads to the DataFrame\n",
    "    group['Spread'] = relative_spreads\n",
    "    return group\n",
    "\n",
    "\n",
    "\n",
    "def get_roll_spread(group):\n",
    "    \"\"\"\n",
    "    Roll (1984) spread estimator based on serial covariance of price changes.\n",
    "    Returns a single spread estimate per stock (not time-varying).\n",
    "    \n",
    "    Roll's spread = 2 * sqrt(-Cov(ΔP_t, ΔP_t-1))\n",
    "    \n",
    "    The Roll estimator assumes that negative serial covariance in returns is \n",
    "    caused by bid-ask bounce. If the covariance is positive, it violates \n",
    "    Roll's model assumptions and we return NaN.\n",
    "    \n",
    "    Reference: Roll, R. (1984). A simple implicit measure of the effective \n",
    "    bid-ask spread in an efficient market. The Journal of Finance, 39(4), 1127-1139.\n",
    "    \"\"\"\n",
    "    group = group.sort_values('Date')\n",
    "    returns = group['Close'].pct_change().dropna()\n",
    "    \n",
    "    if len(returns) < 30:  # Minimum observations for reliable estimate\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate serial covariance (covariance between returns and lagged returns)\n",
    "    cov = returns.cov(returns.shift(1))\n",
    "    \n",
    "    # Roll's spread formula (only valid when covariance is negative)\n",
    "    if pd.notna(cov) and cov < 0:\n",
    "        spread = 2 * np.sqrt(-cov)\n",
    "    else:\n",
    "        # Positive covariance violates Roll's assumptions\n",
    "        spread = np.nan\n",
    "    \n",
    "    return spread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2522e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT 1.2: ADD PERIOD LABELS FOR HOLIDAY ANALYSIS\n",
    "def add_period_labels(df):\n",
    "    \"\"\"\n",
    "    Adds period labels for holiday analysis.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    years = range(df['Date'].dt.year.min(), df['Date'].dt.year.max() + 1)\n",
    "    norway_holidays = holidays.Norway(years=years)\n",
    "    \n",
    "    df['week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['period_label'] = 'control_year'\n",
    "\n",
    "    for yr in years:\n",
    "        christmas_ref_start = pd.Timestamp(f'{yr}-12-15')\n",
    "        christmas_ref_end = pd.Timestamp(f'{yr}-12-31')\n",
    "        christmas_days = pd.bdate_range(start=christmas_ref_start, end=christmas_ref_end, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "        \n",
    "        if len(christmas_days) > 0:\n",
    "            pre_christmas = pd.bdate_range(end=christmas_days[0] - pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            try:\n",
    "                post_christmas_start = pd.Timestamp(f'{yr+1}-01-02')\n",
    "                post_christmas = pd.bdate_range(start=post_christmas_start, periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            except:\n",
    "                post_christmas = pd.bdate_range(start=christmas_days[-1] + pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            \n",
    "            df.loc[df['Date'].isin(pre_christmas), 'period_label'] = 'pre_christmas'\n",
    "            df.loc[df['Date'].isin(christmas_days), 'period_label'] = 'christmas'\n",
    "            df.loc[df['Date'].isin(post_christmas), 'period_label'] = 'post_christmas'\n",
    "\n",
    "    for yr in years:\n",
    "        easter_days = [d for d, h in norway_holidays.items() if ('Påske' in h or h in ['Skjærtorsdag', 'Langfredag']) and d.year == yr]\n",
    "        \n",
    "        if easter_days:\n",
    "            easter_start = min(easter_days)\n",
    "            easter_end = max(easter_days)\n",
    "            pre_easter = pd.bdate_range(end=easter_start - pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            post_easter = pd.bdate_range(start=easter_end + pd.Timedelta(days=1), periods=5, freq='C', holidays=list(norway_holidays.keys()), weekmask='Mon Tue Wed Thu Fri')\n",
    "            df.loc[df['Date'].isin(pre_easter), 'period_label'] = 'pre_easter'\n",
    "            df.loc[df['Date'].isin(post_easter), 'period_label'] = 'post_easter'\n",
    "\n",
    "    df.loc[df['week'].between(28, 30), 'period_label'] = 'summer_holiday'\n",
    "    df.loc[(df['month'].between(6, 8)) & (~df['week'].between(28, 30)), 'period_label'] = 'summer_excl_holiday'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79dba716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analysis will use the 'Corwin-Schultz (2012) Spread'\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 1.3: CHOOSE SPREAD ESTIMATOR (Simplified)\n",
    "user_input = input(\"Which spread estimator do you want to use? (Enter 'CS' or 'AR'): \")\n",
    "\n",
    "# Determine the spread method based on user input\n",
    "if user_input.upper() == 'AR':\n",
    "    METHOD_CHOICE = 'AR'\n",
    "    SPREAD_METHOD_NAME = 'Abdi & Ranaldo (2017) Spread'\n",
    "else:\n",
    "    METHOD_CHOICE = 'CS'\n",
    "    SPREAD_METHOD_NAME = 'Corwin-Schultz (2012) Spread'\n",
    "\n",
    "print(f\"The analysis will use the '{SPREAD_METHOD_NAME}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdba6924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analysis will use the 'FDR' correction method.\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 1.4: CHOOSE MULTIPLE TESTING CORRECTION METHOD\n",
    "\n",
    "correction_input = input(\n",
    "    \"Which multiple testing correction do you want to use for significance?\\n(Enter 'B' or 'FDR')\").strip().lower()\n",
    "\n",
    "if correction_input == 'fdr':\n",
    "    CORRECTION_METHOD = 'FDR'\n",
    "    CORRECTION_METHOD_NAME = 'FDR'\n",
    "elif correction_input == 'B':\n",
    "    CORRECTION_METHOD = 'Bonferroni'\n",
    "    CORRECTION_METHOD_NAME = 'Bonferroni'\n",
    "else:\n",
    "    print(\"Invalid input - using FDR by default.\")\n",
    "    CORRECTION_METHOD = 'FDR'\n",
    "    CORRECTION_METHOD_NAME = 'FDR'\n",
    "\n",
    "print(f\"The analysis will use the '{CORRECTION_METHOD_NAME}' correction method.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010d145d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data for 90 tickers...\n",
      "Date range: 2014-01-01 to 2024-12-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  90 of 90 completed\n",
      "\n",
      "2 Failed downloads:\n",
      "['FLNG.OL']: YFPricesMissingError('possibly delisted; no price data found  (1d 2014-01-01 -> 2024-12-31) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "['ENTRA.OL']: DNSError('Failed to perform, curl: (6) Could not resolve host: query2.finance.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMING DATA STRUCTURE: \n",
      "Removed 38,540 rows with missing price/volume data\n",
      "\n",
      "\n",
      "CALCULATING CORWIN-SCHULTZ SPREADS (DAILY ESTIMATE)\n",
      "Spread calculation complete.\n",
      "\n",
      "\n",
      "SPREAD DISTRIBUTION DIAGNOSTICS - CS\n",
      "\n",
      "Spread Value Distribution:\n",
      "  Total observations:        210,040\n",
      "  Zero values (= 0):         11,521 (5.49%)\n",
      "  Negative values (< 0):     0 (0.00%)\n",
      "  Tiny spreads (0 to 0,01%): 1,180 (0.56%)\n",
      "  Valid spreads (≥ 0,01%):   116,310 (55.38%)\n",
      "  Missing (NaN):             81,029 (38.58%)\n",
      "\n",
      "Spread Percentiles (before filtering):\n",
      "    min: 0.000000\n",
      "     1%: 0.000000\n",
      "     5%: 0.000000\n",
      "    10%: 0.000153\n",
      "    25%: 0.004375\n",
      "    50%: 0.010407\n",
      "    75%: 0.019695\n",
      "    90%: 0.033653\n",
      "    95%: 0.046026\n",
      "    99%: 0.080557\n",
      "    max: 0.408711\n",
      "\n",
      "Median spread: 0.010407 (appears reasonable)\n",
      "DATA QUALITY FILTERING - CS\n",
      "\n",
      "Removed 81,029 rows with NaN spreads\n",
      "Removed 12,701 rows with spreads < 0.01% (1 bp)\n",
      "These are economically meaningless and likely measurement errors\n",
      "\n",
      "Total rows removed: 93,730\n",
      "Remaining observations: 116,310\n",
      "\n",
      "POST-FILTERING SPREAD STATISTICS:\n",
      "  Mean:   0.016663\n",
      "  Median: 0.011825\n",
      "  Std:    0.017365\n",
      "  Min:    0.000100\n",
      "  Max:    0.408711\n",
      "\n",
      "Roll's spread calculation summary:\n",
      "  Tickers with valid spread: 62 / 88\n",
      "  Tickers with invalid spread: 26\n",
      "   - Likely due to positive serial covariance (violates Roll's assumptions)\n",
      "  Average Roll's spread: 0.020222\n",
      "\n",
      "Correlation between Roll and CS average: 0.669\n",
      " - Moderate correlation - spread estimates are reasonably consistent\n",
      "\n",
      "Data saved to: oslo_bors_labelled_data.csv\n",
      "Total observations: 116,310\n",
      "Date range: 2014-01-03 00:00:00 to 2024-12-30 00:00:00\n",
      "Tickers: 88\n",
      "\n",
      " FINAL SPREAD SUMMARY BY CAP GROUP:\n",
      "           Observations      Mean    Median   Std Dev\n",
      "Cap_Group                                            \n",
      "Large Cap         41855  0.012159  0.009567  0.010714\n",
      "Mid Cap           38204  0.017493  0.012296  0.018214\n",
      "Small Cap         36251  0.020989  0.015222  0.021036\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 1.5: MAIN DATA PROCESSING FUNCTION\n",
    "\n",
    "def main():\n",
    "    \n",
    "    all_tickers = [ticker for sublist in TICKERS.values() for ticker in sublist]\n",
    "    print(f\"\\nDownloading data for {len(all_tickers)} tickers...\")\n",
    "    print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "    \n",
    "    raw_data = yf.download(all_tickers, start=START_DATE, end=END_DATE, progress=True, auto_adjust=True)\n",
    "\n",
    "    if raw_data.empty:\n",
    "        print(\"ERROR: No data downloaded\")\n",
    "        return\n",
    "\n",
    "    print(\"TRANSFORMING DATA STRUCTURE: \")\n",
    "    df = raw_data.stack(future_stack=True).reset_index()\n",
    "    df = df.rename(columns={'level_1': 'Ticker'})\n",
    "    \n",
    "    ticker_to_cap = {ticker: cap for cap, tickers in TICKERS.items() for ticker in tickers}\n",
    "    df['Cap_Group'] = df['Ticker'].map(ticker_to_cap)\n",
    "    \n",
    "    initial_len = len(df)\n",
    "    df.dropna(subset=['Open', 'High', 'Low', 'Close', 'Volume', 'Cap_Group'], inplace=True)\n",
    "    print(f\"Removed {initial_len - len(df):,} rows with missing price/volume data\")\n",
    "\n",
    "    # Spread Calculation\n",
    "    print(\"\\n\")\n",
    "    if METHOD_CHOICE == 'CS':\n",
    "        print(\"CALCULATING CORWIN-SCHULTZ SPREADS (DAILY ESTIMATE)\")\n",
    "        df = df.groupby('Ticker', group_keys=False).apply(corwin_schultz_spread)\n",
    "    elif METHOD_CHOICE == 'AR':\n",
    "        print(\"CALCULATING ABDI & RANALDO (2017) 'BAR' SPREADS\")\n",
    "        df = df.groupby('Ticker', group_keys=False).apply(abdi_ranaldo_spread)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method choice. Please choose 'CS' or 'AR'.\")\n",
    "    print(\"Spread calculation complete.\")\n",
    "\n",
    "    # CRITICAL FIX: COMPREHENSIVE SPREAD DIAGNOSTICS\n",
    "    print(\"\\n\")\n",
    "    print(f\"SPREAD DISTRIBUTION DIAGNOSTICS - {METHOD_CHOICE}\")\n",
    "    \n",
    "    # Count spread values by category\n",
    "    total_obs = len(df)\n",
    "    zero_spreads = (df['Spread'] == 0).sum()\n",
    "    negative_spreads = (df['Spread'] < 0).sum()\n",
    "    tiny_spreads = ((df['Spread'] > 0) & (df['Spread'] < 0.0001)).sum()\n",
    "    valid_spreads = (df['Spread'] >= 0.0001).sum()\n",
    "    nan_spreads = df['Spread'].isna().sum()\n",
    "    \n",
    "    print(f\"\\nSpread Value Distribution:\")\n",
    "    print(f\"  Total observations:        {total_obs:,}\")\n",
    "    print(f\"  Zero values (= 0):         {zero_spreads:,} ({zero_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Negative values (< 0):     {negative_spreads:,} ({negative_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Tiny spreads (0 to 0,01%): {tiny_spreads:,} ({tiny_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Valid spreads (≥ 0,01%):   {valid_spreads:,} ({valid_spreads/total_obs*100:.2f}%)\")\n",
    "    print(f\"  Missing (NaN):             {nan_spreads:,} ({nan_spreads/total_obs*100:.2f}%)\")\n",
    "    \n",
    "    # Percentile analysis\n",
    "    print(f\"\\nSpread Percentiles (before filtering):\")\n",
    "    percentiles = df['Spread'].describe(percentiles=[.01, .05, .10, .25, .50, .75, .90, .95, .99])\n",
    "    for stat in ['min', '1%', '5%', '10%', '25%', '50%', '75%', '90%', '95%', '99%', 'max']:\n",
    "        if stat in percentiles.index:\n",
    "            print(f\"  {stat:>5}: {percentiles[stat]:.6f}\")\n",
    "    \n",
    "    # Warning if median is zero or very low\n",
    "    median_spread = df['Spread'].median()\n",
    "    if median_spread == 0:\n",
    "        print(\"\\nWARNING: Median spread is ZERO!\")\n",
    "        print(\"   - This indicates a data quality issue with the spread estimator.\")\n",
    "        print(\"   - Most observations have zero spread, which is economically unrealistic.\")\n",
    "    elif median_spread < 0.0001:\n",
    "        print(f\"\\nWARNING: Median spread is very low ({median_spread:.6f})\")\n",
    "        print(\"   - Many spreads may be economically meaningless.\")\n",
    "    else:\n",
    "        print(f\"\\nMedian spread: {median_spread:.6f} (appears reasonable)\")\n",
    "\n",
    "    # Data Quality Check and Cleaning\n",
    "    print(f\"DATA QUALITY FILTERING - {METHOD_CHOICE}\")\n",
    "    \n",
    "    spread_before = len(df)\n",
    "    \n",
    "    # Remove NaN spreads\n",
    "    df = df.dropna(subset=['Spread'])\n",
    "    nan_removed = spread_before - len(df)\n",
    "    print(f\"\\nRemoved {nan_removed:,} rows with NaN spreads\")\n",
    "    \n",
    "    # CRITICAL FIX: Remove economically meaningless spreads\n",
    "    # Spreads below 0.01% (1 basis point) are likely measurement errors \n",
    "    spread_before_filter = len(df)\n",
    "    df = df[df['Spread'] >= 0.0001]\n",
    "    tiny_removed = spread_before_filter - len(df)\n",
    "    print(f\"Removed {tiny_removed:,} rows with spreads < 0.01% (1 bp)\")\n",
    "    print(f\"These are economically meaningless and likely measurement errors\")\n",
    "    \n",
    "    print(f\"\\nTotal rows removed: {spread_before - len(df):,}\")\n",
    "    print(f\"Remaining observations: {len(df):,}\")\n",
    "    \n",
    "    # Post-filtering diagnostics\n",
    "    print(f\"\\nPOST-FILTERING SPREAD STATISTICS:\")\n",
    "    print(f\"  Mean:   {df['Spread'].mean():.6f}\")\n",
    "    print(f\"  Median: {df['Spread'].median():.6f}\")\n",
    "    print(f\"  Std:    {df['Spread'].std():.6f}\")\n",
    "    print(f\"  Min:    {df['Spread'].min():.6f}\")\n",
    "    print(f\"  Max:    {df['Spread'].max():.6f}\")\n",
    "\n",
    "    # Roll's Spread Calculation\n",
    "    roll_spreads = df.groupby('Ticker').apply(get_roll_spread)\n",
    "    df['RollsSpread'] = df['Ticker'].map(roll_spreads)\n",
    "    \n",
    "    valid_rolls = roll_spreads.notna().sum()\n",
    "    total_tickers = df['Ticker'].nunique()\n",
    "    invalid_rolls = total_tickers - valid_rolls\n",
    "    \n",
    "    print(f\"\\nRoll's spread calculation summary:\")\n",
    "    print(f\"  Tickers with valid spread: {valid_rolls} / {total_tickers}\")\n",
    "    if invalid_rolls > 0:\n",
    "        print(f\"  Tickers with invalid spread: {invalid_rolls}\")\n",
    "        print(f\"   - Likely due to positive serial covariance (violates Roll's assumptions)\")\n",
    "    print(f\"  Average Roll's spread: {roll_spreads.mean():.6f}\")\n",
    "    \n",
    "    # Compare Roll with CS/AR\n",
    "    avg_cs_ar = df.groupby('Ticker')['Spread'].mean()\n",
    "    comparison = pd.DataFrame({\n",
    "        'Roll': roll_spreads,\n",
    "        'CS_AR_Avg': avg_cs_ar\n",
    "    }).dropna()\n",
    "    \n",
    "    if len(comparison) > 0:\n",
    "        correlation = comparison.corr().iloc[0, 1]\n",
    "        print(f\"\\nCorrelation between Roll and {METHOD_CHOICE} average: {correlation:.3f}\")\n",
    "        if correlation > 0.7:\n",
    "            print(f\" - Strong correlation indicates reliable spread estimates!\")\n",
    "        elif correlation > 0.5:\n",
    "            print(f\" - Moderate correlation - spread estimates are reasonably consistent\")\n",
    "        else:\n",
    "            print(f\"Low correlation - may indicate measurement issues\")\n",
    "\n",
    "    # Period Labeling and Final Export\n",
    "    df = add_period_labels(df)\n",
    "    \n",
    "    \n",
    "    final_cols = ['Date', 'Ticker', 'Cap_Group', 'Open', 'High', 'Low', 'Close', \n",
    "                  'Volume', 'Spread', 'RollsSpread', 'period_label']\n",
    "    df_final = df[[col for col in final_cols if col in df.columns]]\n",
    "    df_final.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\nData saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Total observations: {len(df_final):,}\")\n",
    "    print(f\"Date range: {df_final['Date'].min()} to {df_final['Date'].max()}\")\n",
    "    print(f\"Tickers: {df_final['Ticker'].nunique()}\")\n",
    "    \n",
    "    # Final summary by cap group\n",
    "    print(f\"\\n FINAL SPREAD SUMMARY BY CAP GROUP:\")\n",
    "    summary = df_final.groupby('Cap_Group')['Spread'].agg(['count', 'mean', 'median', 'std'])\n",
    "    summary.columns = ['Observations', 'Mean', 'Median', 'Std Dev']\n",
    "    print(summary.to_string())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e25dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METHODOLOGICAL SPLIT-SAMPLE DESIGN IMPLEMENTED\n",
      "In-Sample Data (2014-2019):     54,135 observations (Used for analysis/regression)\n",
      "Out-of-Sample Data (2020-2024): 62,175 observations (Used ONLY for backtesting)\n",
      "\n",
      "Total observations (In-Sample): 54,135\n",
      "Date range: 2014-01-03 00:00:00 to 2019-12-30 00:00:00\n",
      "Unique tickers: 77\n",
      "Trading days: 1,499\n",
      "\n",
      "Cohen's d Effect Sizes (vs. Control Period):\n",
      " Summer Holiday: d = -0.1368\n",
      " Christmas: d = 0.1570\n",
      " Pre-Easter: d = -0.0308\n",
      " Post-Easter: d = -0.0033\n",
      "\n",
      "Interpretation: |d| > 0.5 = medium to large effect\n",
      "HARRIS FRAMEWORK: DEALER BEHAVIOR HYPOTHESIS\n",
      "\n",
      "Testing Harris's prediction: Spreads should increase when volume decreases\n",
      "(Lower dealer participation and higher inventory risk)\n",
      "\n",
      "Spread by Volume Quartile:\n",
      "                 Spread_Mean  Spread_Median  N_Obs    Avg_Volume\n",
      "Volume_Quartile                                                 \n",
      "Q1_Lowest           0.016534       0.010718  13534  5.130461e+03\n",
      "Q2_Low              0.015805       0.011168  13534  6.148921e+04\n",
      "Q3_High             0.013630       0.010487  13533  2.914159e+05\n",
      "Q4_Highest          0.013563       0.009894  13534  2.512188e+06\n",
      "\n",
      "Harris Hypothesis Test:\n",
      " H0: Spread(Low Volume) = Spread(High Volume)\n",
      " H1: Spread(Low Volume) > Spread(High Volume)\n",
      " Mean spread Q1 (low vol): 0.016534\n",
      " Mean spread Q4 (high vol): 0.013563\n",
      " Difference: 0.002971\n",
      " t-statistic: 15.2271\n",
      " p-value: 0.0000\n",
      " Cohen's d: 0.1851\n",
      " ✓ RESULT: Harris's prediction CONFIRMED (p < 0.05)\n",
      " - Spreads are wider when volume is low (dealer participation reduced)\n",
      "SUMMARY BY CAP GROUP\n",
      "           Spread_Mean  Spread_Median  Spread_Std   Volume_Mean  \\\n",
      "Cap_Group                                                         \n",
      "Large Cap     0.011279       0.008996    0.009518  1.190780e+06   \n",
      "Mid Cap       0.016013       0.011071    0.016793  4.367910e+05   \n",
      "Small Cap     0.018948       0.013665    0.019230  3.486700e+05   \n",
      "\n",
      "           Volume_Median  N_Stocks  \n",
      "Cap_Group                           \n",
      "Large Cap       423292.0        28  \n",
      "Mid Cap          39575.0        26  \n",
      "Small Cap        48604.5        23  \n",
      "\n",
      " HARRIS INTERPRETATION:\n",
      " Large Cap stocks have narrower spreads due to:\n",
      " - Higher competitive market making (Harris Chapter 6)\n",
      " - Lower inventory risk for dealers\n",
      " - Greater participation rates (Harris Chapter 21)\n",
      "SUMMARY BY PERIOD\n",
      "                     Spread_Mean  Spread_Median    Volume_Mean  Volume_Median  \\\n",
      "period_label                                                                    \n",
      "christmas               0.017456       0.011798  761404.803240       128548.0   \n",
      "control_year            0.014975       0.010543  737102.818046       131220.0   \n",
      "post_christmas          0.017469       0.012414  759563.855204       133376.0   \n",
      "post_easter             0.014923       0.011345  719232.690052       163954.0   \n",
      "pre_christmas           0.016377       0.011121  683680.949866       125030.0   \n",
      "pre_easter              0.014495       0.010445  627659.034113       113534.5   \n",
      "summer_excl_holiday     0.014354       0.010213  696336.994017       123314.0   \n",
      "summer_holiday          0.012873       0.009899  580166.968088        98922.0   \n",
      "\n",
      "                     N_Obs  \n",
      "period_label                \n",
      "christmas             2099  \n",
      "control_year         34025  \n",
      "post_christmas         884  \n",
      "post_easter            955  \n",
      "pre_christmas         1117  \n",
      "pre_easter            1026  \n",
      "summer_excl_holiday  10864  \n",
      "summer_holiday        3165  \n",
      "CHANGES VS CONTROL PERIOD\n",
      "                     Spread_Mean  Spread_Change_%    Volume_Mean  \\\n",
      "period_label                                                       \n",
      "christmas               0.017456            16.57  761404.803240   \n",
      "control_year            0.014975             0.00  737102.818046   \n",
      "post_christmas          0.017469            16.65  759563.855204   \n",
      "post_easter             0.014923            -0.35  719232.690052   \n",
      "pre_christmas           0.016377             9.36  683680.949866   \n",
      "pre_easter              0.014495            -3.21  627659.034113   \n",
      "summer_excl_holiday     0.014354            -4.15  696336.994017   \n",
      "summer_holiday          0.012873           -14.04  580166.968088   \n",
      "\n",
      "                     Volume_Change_%  \n",
      "period_label                          \n",
      "christmas                       3.30  \n",
      "control_year                    0.00  \n",
      "post_christmas                  3.05  \n",
      "post_easter                    -2.42  \n",
      "pre_christmas                  -7.25  \n",
      "pre_easter                    -14.85  \n",
      "summer_excl_holiday            -5.53  \n",
      "summer_holiday                -21.29  \n",
      "PRACTICAL IMPLICATIONS: TRADING COST ANALYSIS\n",
      "\n",
      "Spread difference (Summer vs. Control): -0.002102\n",
      "Average stock price assumed: 100 NOK\n",
      "\n",
      "Cost Impact per Trade (one-way):\n",
      "    Trade Size (NOK)    Control Cost     Summer Cost         Savings\n",
      "----------------------------------------------------------------------\n",
      "             100,000         748.75 NOK         643.65 NOK         105.10 NOK\n",
      "             500,000        3743.75 NOK        3218.25 NOK         525.50 NOK\n",
      "           1,000,000        7487.50 NOK        6436.50 NOK        1051.00 NOK\n",
      "           5,000,000       37437.50 NOK       32182.50 NOK        5255.00 NOK\n",
      "\n",
      "INTERPRETATION:\n",
      " Trading during summer holidays saves approximately 14.0%\n",
      " on transaction costs due to narrower spreads.\n",
      "\n",
      "Applying Bonferroni Correction\n",
      "Total tests performed: 21\n",
      "Bonferroni-corrected alpha level: 0.00238\n",
      "\n",
      "Applying Benjamini-Hochberg (FDR) Correction\n",
      "FDR correction applied.\n",
      "\n",
      "\n",
      "Bonferroni Corrected P-Values (Spread)\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas  \\\n",
      "Large Cap      0.0374          1.0         1.0000     0.0375          0.0339   \n",
      "Mid Cap        1.0000          1.0         0.8222     0.0010          0.9129   \n",
      "Small Cap      1.0000          1.0         1.0000     0.0468          0.4992   \n",
      "\n",
      "           summer_holiday  summer_excl_holiday  \n",
      "Large Cap          0.0001               1.0000  \n",
      "Mid Cap            0.0004               1.0000  \n",
      "Small Cap          0.0000               0.0022  \n",
      "\n",
      "FDR Corrected P-Values (Spread)\n",
      "           pre_easter  post_easter  pre_christmas  christmas  post_christmas  \\\n",
      "Large Cap      0.0047       0.2968         0.9603     0.0047          0.0047   \n",
      "Mid Cap        0.4116       0.7012         0.0747     0.0002          0.0761   \n",
      "Small Cap      0.6469       0.6793         0.1719     0.0052          0.0499   \n",
      "\n",
      "           summer_holiday  summer_excl_holiday  \n",
      "Large Cap          0.0001               0.1763  \n",
      "Mid Cap            0.0001               0.2968  \n",
      "Small Cap          0.0000               0.0004  \n",
      "Sample size after cleaning: 5,411 observations\n",
      "\n",
      "All outputs saved to 'output/' folder\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 2: DESCRIPTIVE STATISTICS AND VISUALIZATION\n",
    "# Load Data\n",
    "try:\n",
    "    df = pd.read_csv('oslo_bors_labelled_data.csv', parse_dates=['Date'])\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'oslo_bors_labelled_data.csv' not found.\")\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "\n",
    "# CRITICAL STEP: SPLIT SAMPLE DESIGN \n",
    "# Split data into In-Sample (Discovery) and Out-of-Sample (Validation)\n",
    "# In-Sample: 2014-2019 (Used for descriptive stats, regressions, establishing the effect)\n",
    "# Out-of-Sample: 2020-2024 (Used ONLY for backtesting trading strategies)\n",
    "\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "\n",
    "# Create the two distinct datasets\n",
    "df_in_sample = df[df['Year'] <= 2019].copy()\n",
    "df_out_of_sample = df[df['Year'] >= 2020].copy()\n",
    "\n",
    "\n",
    "print(\"METHODOLOGICAL SPLIT-SAMPLE DESIGN IMPLEMENTED\")\n",
    "print(f\"In-Sample Data (2014-2019):     {len(df_in_sample):,} observations (Used for analysis/regression)\")\n",
    "print(f\"Out-of-Sample Data (2020-2024): {len(df_out_of_sample):,} observations (Used ONLY for backtesting)\")\n",
    "\n",
    "\n",
    "# IMPORTANT: Overwrite 'df' to be only in-sample data for the rest of the analysis (Script 2-5)\n",
    "# This is to ensure that tables and graphs do not \"see\" the future.\n",
    "df_full_backup = df.copy()\n",
    "df = df_in_sample \n",
    "\n",
    "\n",
    "# Dataset overview (In-Sample)\n",
    "print(f\"\\nTotal observations (In-Sample): {len(df):,}\")\n",
    "try:\n",
    "    print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "    print(f\"Unique tickers: {df['Ticker'].nunique()}\")\n",
    "    print(f\"Trading days: {df['Date'].nunique():,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# CRITICAL ADDITION: COHEN'S D EFFECT SIZE CALCULATION\n",
    "\n",
    "\n",
    "def calculate_cohens_d(group1, group2):\n",
    "    if len(group1) < 2 or len(group2) < 2: return np.nan\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    if pooled_std == 0: return 0\n",
    "    d = (group1.mean() - group2.mean()) / pooled_std\n",
    "    return d\n",
    "\n",
    "\n",
    "# Calculate effect sizes for key holiday periods\n",
    "control = df[df['period_label'] == 'control_year']['Spread']\n",
    "summer = df[df['period_label'] == 'summer_holiday']['Spread']\n",
    "christmas = df[df['period_label'] == 'christmas']['Spread']\n",
    "easter_pre = df[df['period_label'] == 'pre_easter']['Spread']\n",
    "easter_post = df[df['period_label'] == 'post_easter']['Spread']\n",
    "\n",
    "\n",
    "print(\"\\nCohen's d Effect Sizes (vs. Control Period):\")\n",
    "print(f\" Summer Holiday: d = {calculate_cohens_d(summer, control):.4f}\")\n",
    "print(f\" Christmas: d = {calculate_cohens_d(christmas, control):.4f}\")\n",
    "print(f\" Pre-Easter: d = {calculate_cohens_d(easter_pre, control):.4f}\")\n",
    "print(f\" Post-Easter: d = {calculate_cohens_d(easter_post, control):.4f}\")\n",
    "print(\"\\nInterpretation: |d| > 0.5 = medium to large effect\")\n",
    "\n",
    "\n",
    "# CRITICAL ADDITION: HARRIS FRAMEWORK - DEALER BEHAVIOR HYPOTHESIS TEST\n",
    "print(\"HARRIS FRAMEWORK: DEALER BEHAVIOR HYPOTHESIS\")\n",
    "print(\"\\nTesting Harris's prediction: Spreads should increase when volume decreases\")\n",
    "print(\"(Lower dealer participation and higher inventory risk)\")\n",
    "\n",
    "\n",
    "df['Volume_Quartile'] = pd.qcut(df['Volume'], q=4, labels=['Q1_Lowest', 'Q2_Low', 'Q3_High', 'Q4_Highest'], duplicates='drop')\n",
    "\n",
    "\n",
    "spread_by_volume = df.groupby('Volume_Quartile', observed=True).agg({\n",
    "    'Spread': ['mean', 'median', 'count'],\n",
    "    'Volume': 'mean'\n",
    "}).round(6)\n",
    "spread_by_volume.columns = ['Spread_Mean', 'Spread_Median', 'N_Obs', 'Avg_Volume']\n",
    "\n",
    "\n",
    "print(\"\\nSpread by Volume Quartile:\")\n",
    "print(spread_by_volume)\n",
    "\n",
    "\n",
    "q1_spread = df[df['Volume_Quartile'] == 'Q1_Lowest']['Spread']\n",
    "q4_spread = df[df['Volume_Quartile'] == 'Q4_Highest']['Spread']\n",
    "t_stat, p_val = ttest_ind(q1_spread.dropna(), q4_spread.dropna(), equal_var=False)\n",
    "cohens_d_volume = calculate_cohens_d(q1_spread.dropna(), q4_spread.dropna())\n",
    "\n",
    "\n",
    "print(f\"\\nHarris Hypothesis Test:\")\n",
    "print(f\" H0: Spread(Low Volume) = Spread(High Volume)\")\n",
    "print(f\" H1: Spread(Low Volume) > Spread(High Volume)\")\n",
    "try:\n",
    "    print(f\" Mean spread Q1 (low vol): {q1_spread.mean():.6f}\")\n",
    "    print(f\" Mean spread Q4 (high vol): {q4_spread.mean():.6f}\")\n",
    "    print(f\" Difference: {q1_spread.mean() - q4_spread.mean():.6f}\")\n",
    "    print(f\" t-statistic: {t_stat:.4f}\")\n",
    "    print(f\" p-value: {p_val:.4f}\")\n",
    "    print(f\" Cohen's d: {cohens_d_volume:.4f}\")\n",
    "\n",
    "\n",
    "    if p_val < 0.05 and q1_spread.mean() > q4_spread.mean():\n",
    "        print(\" ✓ RESULT: Harris's prediction CONFIRMED (p < 0.05)\")\n",
    "        print(\" - Spreads are wider when volume is low (dealer participation reduced)\")\n",
    "    elif p_val < 0.05 and q1_spread.mean() < q4_spread.mean():\n",
    "        print(\" ✗ RESULT: Harris's prediction CONTRADICTED (p < 0.05)\")\n",
    "        print(\" - Modern electronic markets may behave differently than Harris predicted\")\n",
    "    else:\n",
    "        print(\" -  RESULT: No significant relationship (p >= 0.05)\")\n",
    "except:\n",
    "    print(\"Insufficient data for Harris test.\")\n",
    "\n",
    "\n",
    "\n",
    "# Summary by cap group\n",
    "print(\"SUMMARY BY CAP GROUP\")\n",
    "summary_cap = df.groupby('Cap_Group').agg({\n",
    "    'Spread': ['mean', 'median', 'std'],\n",
    "    'Volume': ['mean', 'median'],\n",
    "    'Ticker': 'nunique'\n",
    "}).round(6)\n",
    "summary_cap.columns = ['Spread_Mean', 'Spread_Median', 'Spread_Std', 'Volume_Mean', 'Volume_Median', 'N_Stocks']\n",
    "print(summary_cap)\n",
    "\n",
    "\n",
    "\n",
    "# HARRIS INTERPRETATION\n",
    "print(\"\\n HARRIS INTERPRETATION:\")\n",
    "print(\" Large Cap stocks have narrower spreads due to:\")\n",
    "print(\" - Higher competitive market making (Harris Chapter 6)\")\n",
    "print(\" - Lower inventory risk for dealers\")\n",
    "print(\" - Greater participation rates (Harris Chapter 21)\")\n",
    "\n",
    "\n",
    "\n",
    "# Summary by period\n",
    "print(\"SUMMARY BY PERIOD\")\n",
    "summary_period = df.groupby('period_label').agg({\n",
    "    'Spread': ['mean', 'median'],\n",
    "    'Volume': ['mean', 'median'],\n",
    "    'Date': 'count'\n",
    "}).round(6)\n",
    "summary_period.columns = ['Spread_Mean', 'Spread_Median', 'Volume_Mean', 'Volume_Median', 'N_Obs']\n",
    "print(summary_period)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate percentage changes vs control\n",
    "try:\n",
    "    control_spread = summary_period.loc['control_year', 'Spread_Mean']\n",
    "    control_volume = summary_period.loc['control_year', 'Volume_Mean']\n",
    "    summary_period['Spread_Change_%'] = ((summary_period['Spread_Mean'] - control_spread) / control_spread * 100).round(2)\n",
    "    summary_period['Volume_Change_%'] = ((summary_period['Volume_Mean'] - control_volume) / control_volume * 100).round(2)\n",
    "\n",
    "\n",
    "    print(\"CHANGES VS CONTROL PERIOD\")\n",
    "    print(summary_period[['Spread_Mean', 'Spread_Change_%', 'Volume_Mean', 'Volume_Change_%']])\n",
    "except KeyError:\n",
    "    print(\"Control year not found in summary.\")\n",
    "\n",
    "\n",
    "\n",
    "summary_cap.to_csv('output/summary_by_cap.csv')\n",
    "summary_period.to_csv('output/summary_by_period.csv')\n",
    "\n",
    "\n",
    "\n",
    "# CRITICAL ADDITION: PRACTICAL IMPLICATIONS (NOK COST SAVINGS)\n",
    "print(\"PRACTICAL IMPLICATIONS: TRADING COST ANALYSIS\")\n",
    "\n",
    "\n",
    "trade_sizes = [100000, 500000, 1000000, 5000000] # NOK\n",
    "avg_stock_price = 100 # We assume 100 NOK per share\n",
    "\n",
    "\n",
    "try:\n",
    "    spread_control = summary_period.loc['control_year', 'Spread_Mean']\n",
    "    spread_summer = summary_period.loc['summer_holiday', 'Spread_Mean']\n",
    "    spread_diff = spread_summer - spread_control\n",
    "\n",
    "\n",
    "    print(f\"\\nSpread difference (Summer vs. Control): {spread_diff:.6f}\")\n",
    "    print(f\"Average stock price assumed: {avg_stock_price} NOK\\n\")\n",
    "\n",
    "\n",
    "    print(\"Cost Impact per Trade (one-way):\")\n",
    "    print(f\"{'Trade Size (NOK)':>20} {'Control Cost':>15} {'Summer Cost':>15} {'Savings':>15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "    for trade_size in trade_sizes:\n",
    "        shares = trade_size / avg_stock_price\n",
    "        cost_control = shares * avg_stock_price * spread_control / 2 # Half-spread for one-way\n",
    "        cost_summer = shares * avg_stock_price * spread_summer / 2\n",
    "        savings = cost_control - cost_summer\n",
    "        print(f\"{trade_size:>20,} {cost_control:>14.2f} NOK {cost_summer:>14.2f} NOK {savings:>14.2f} NOK\")\n",
    "\n",
    "\n",
    "    print(\"\\nINTERPRETATION:\")\n",
    "    if spread_diff < 0:\n",
    "        print(f\" Trading during summer holidays saves approximately {abs(spread_diff/spread_control)*100:.1f}%\")\n",
    "        print(f\" on transaction costs due to narrower spreads.\")\n",
    "    else:\n",
    "        print(f\" Trading during summer holidays costs approximately {(spread_diff/spread_control)*100:.1f}% more\")\n",
    "        print(f\" due to wider spreads (consistent with Harris's dealer hypothesis).\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# FIGURE 1: CHRISTMAS & EASTER ANALYSIS\n",
    "\n",
    "christmas_easter_periods = ['control_year', 'pre_easter', 'post_easter',\n",
    "                            'pre_christmas', 'christmas', 'post_christmas']\n",
    "df_ce = df[df['period_label'].isin(christmas_easter_periods)].copy()\n",
    "df_ce['period_label'] = pd.Categorical(df_ce['period_label'], \n",
    "                                       categories=christmas_easter_periods, \n",
    "                                       ordered=True)\n",
    "\n",
    "# FIGURE 1A (UPDATED: Labels to the RIGHT)\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.pointplot(x='period_label', y='Spread', hue='Cap_Group', data=df_ce,\n",
    "              hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "              dodge=True,\n",
    "              palette='viridis',\n",
    "              errorbar='ci',\n",
    "              capsize=0.1)\n",
    "\n",
    "means_1a = df_ce.groupby(['period_label', 'Cap_Group'], observed=False)['Spread'].mean()\n",
    "for i, period in enumerate(christmas_easter_periods):\n",
    "    for j, cap in enumerate(['Large Cap', 'Mid Cap', 'Small Cap']):\n",
    "        try:\n",
    "            val = means_1a.loc[period, cap]\n",
    "            x_pos = i + (j - 1) * 0.15\n",
    "            \n",
    "            if cap == 'Large Cap':\n",
    "                x_offset = 30\n",
    "            else:\n",
    "                x_offset = 8\n",
    "            \n",
    "            ax.annotate(f'{val:.3f}', xy=(x_pos, val), xytext=(x_offset, 0), \n",
    "                        textcoords='offset points', ha='left', va='center', \n",
    "                        fontweight='bold', fontsize=11, color='black',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "plt.title(f'Mean Spread (Christmas & Easter Periods) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Holiday Periods', fontsize=14)\n",
    "plt.ylabel('Mean Relative Spread', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_1A_CE_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 1B\n",
    "volume_ce = df_ce.groupby(['period_label', 'Cap_Group'], observed=False)['Volume'].mean().reset_index()\n",
    "volume_ce['Volume_millions'] = volume_ce['Volume'] / 1e6\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x='period_label', y='Volume_millions', hue='Cap_Group', data=volume_ce,\n",
    "            hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "            palette='magma',\n",
    "            errorbar='ci')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f', padding=5, fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.title(f'Mean Daily Volume (Christmas & Easter Periods) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Holiday Periods', fontsize=14)\n",
    "plt.ylabel('Mean Daily Volume (Million Shares)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_1B_CE_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 2: SUMMER ANALYSIS\n",
    "summer_periods = ['control_year', 'summer_excl_holiday', 'summer_holiday']\n",
    "df_summer = df[df['period_label'].isin(summer_periods)].copy()\n",
    "df_summer['period_label'] = pd.Categorical(df_summer['period_label'], \n",
    "                                           categories=summer_periods, \n",
    "                                           ordered=True)\n",
    "\n",
    "# FIGURE 2A\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = sns.pointplot(x='period_label', y='Spread', hue='Cap_Group', data=df_summer,\n",
    "              hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "              dodge=True,\n",
    "              palette='viridis',\n",
    "              errorbar='ci',\n",
    "              capsize=0.1)\n",
    "\n",
    "means_2a = df_summer.groupby(['period_label', 'Cap_Group'], observed=False)['Spread'].mean()\n",
    "for i, period in enumerate(summer_periods):\n",
    "    for j, cap in enumerate(['Large Cap', 'Mid Cap', 'Small Cap']):\n",
    "        try:\n",
    "            val = means_2a.loc[period, cap]\n",
    "            x_pos = i + (j - 1) * 0.15\n",
    "            \n",
    "            if cap == 'Large Cap':\n",
    "                x_offset = 30\n",
    "            else:\n",
    "                x_offset = 8\n",
    "            \n",
    "            ax.annotate(f'{val:.3f}', xy=(x_pos, val), xytext=(x_offset, 0), \n",
    "                        textcoords='offset points', ha='left', va='center', \n",
    "                        fontweight='bold', fontsize=11, color='black',\n",
    "                        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "plt.title(f'Mean Spread (Summer Holiday) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Summer Periods', fontsize=14)\n",
    "plt.ylabel('Mean Relative Spread', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_2A_Summer_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 2B\n",
    "volume_summer = df_summer.groupby(['period_label', 'Cap_Group'], observed=False)['Volume'].mean().reset_index()\n",
    "volume_summer['Volume_millions'] = volume_summer['Volume'] / 1e6\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = sns.barplot(x='period_label', y='Volume_millions', hue='Cap_Group', data=volume_summer,\n",
    "            hue_order=['Large Cap', 'Mid Cap', 'Small Cap'],\n",
    "            palette='magma',\n",
    "            errorbar='ci')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f', padding=5, fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.title(f'Mean Daily Volume (Summer Holiday) | {SPREAD_METHOD_NAME}', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Summer Periods', fontsize=14)\n",
    "plt.ylabel('Mean Daily Volume (Million Shares)', fontsize=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Cap Group', title_fontsize=12, fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_2B_Summer_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 3: P-VALUE HEATMAPS WITH MULTIPLE TESTING CORRECTION\n",
    "test_periods = ['pre_easter', 'post_easter',\n",
    "                'pre_christmas', 'christmas', 'post_christmas',\n",
    "                'summer_holiday', 'summer_excl_holiday']\n",
    "\n",
    "\n",
    "# Ensure we only test periods that actually exist in the data\n",
    "available_periods = df['period_label'].unique()\n",
    "test_periods = [p for p in test_periods if p in available_periods]\n",
    "\n",
    "\n",
    "p_values_spread = {}\n",
    "p_values_volume = {}\n",
    "all_p_spread = []\n",
    "all_p_volume = []\n",
    "test_info = []\n",
    "\n",
    "\n",
    "# We need a temporary DF that includes all relevant periods + control\n",
    "# (Previously df_ce only had Christmas/Easter)\n",
    "periods_for_heatmap = ['control_year'] + test_periods\n",
    "df_heatmap = df[df['period_label'].isin(periods_for_heatmap)].copy()\n",
    "\n",
    "\n",
    "for group in ['Large Cap', 'Mid Cap', 'Small Cap']:\n",
    "    p_values_spread[group] = {}\n",
    "    p_values_volume[group] = {}\n",
    "    \n",
    "    control = df_heatmap[(df_heatmap['period_label'] == 'control_year') & (df_heatmap['Cap_Group'] == group)]\n",
    "    for period in test_periods:\n",
    "        event = df_heatmap[(df_heatmap['period_label'] == period) & (df_heatmap['Cap_Group'] == group)]\n",
    "        if len(event) > 1 and len(control) > 1:\n",
    "            _, p_spread = ttest_ind(event['Spread'].dropna(), \n",
    "                                    control['Spread'].dropna(), \n",
    "                                    equal_var=False)\n",
    "            _, p_volume = ttest_ind(event['Volume'].dropna(), \n",
    "                                    control['Volume'].dropna(), \n",
    "                                    equal_var=False)\n",
    "            all_p_spread.append(p_spread)\n",
    "            all_p_volume.append(p_volume)\n",
    "            test_info.append((group, period))\n",
    "            p_values_spread[group][period] = p_spread\n",
    "            p_values_volume[group][period] = p_volume\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nApplying Bonferroni Correction\")\n",
    "print(f'Total tests performed: {len(all_p_spread)}')\n",
    "if len(all_p_spread) > 0:\n",
    "    print(f'Bonferroni-corrected alpha level: {0.05 / len(all_p_spread):.5f}\\n')\n",
    "\n",
    "\n",
    "    reject_spread_bonf, pvals_corrected_spread_bonf, _, _ = multipletests(\n",
    "        all_p_spread, alpha=0.05, method='bonferroni'\n",
    "    )\n",
    "    reject_volume_bonf, pvals_corrected_volume_bonf, _, _ = multipletests(\n",
    "        all_p_volume, alpha=0.05, method='bonferroni'\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Applying Benjamini-Hochberg (FDR) Correction\")\n",
    "    reject_spread_fdr, pvals_corrected_spread_fdr, _, _ = multipletests(\n",
    "        all_p_spread, alpha=0.05, method='fdr_bh'\n",
    "    )\n",
    "    reject_volume_fdr, pvals_corrected_volume_fdr, _, _ = multipletests(\n",
    "        all_p_volume, alpha=0.05, method='fdr_bh'\n",
    "    )\n",
    "    print(\"FDR correction applied.\\n\")\n",
    "\n",
    "\n",
    "    p_values_spread_df = pd.DataFrame(p_values_spread).T\n",
    "    p_values_volume_df = pd.DataFrame(p_values_volume).T\n",
    "\n",
    "\n",
    "    p_values_corrected_df_spread_bonf = pd.DataFrame(index=p_values_spread_df.index, columns=p_values_spread_df.columns)\n",
    "    p_values_corrected_df_volume_bonf = pd.DataFrame(index=p_values_volume_df.index, columns=p_values_volume_df.columns)\n",
    "    p_values_corrected_df_spread_fdr = pd.DataFrame(index=p_values_spread_df.index, columns=p_values_spread_df.columns)\n",
    "    p_values_corrected_df_volume_fdr = pd.DataFrame(index=p_values_volume_df.index, columns=p_values_volume_df.columns)\n",
    "\n",
    "\n",
    "    for i, (group, period) in enumerate(test_info):\n",
    "        p_values_corrected_df_spread_bonf.loc[group, period] = pvals_corrected_spread_bonf[i]\n",
    "        p_values_corrected_df_volume_bonf.loc[group, period] = pvals_corrected_volume_bonf[i]\n",
    "        p_values_corrected_df_spread_fdr.loc[group, period] = pvals_corrected_spread_fdr[i]\n",
    "        p_values_corrected_df_volume_fdr.loc[group, period] = pvals_corrected_volume_fdr[i]\n",
    "\n",
    "\n",
    "    print(\"\\nBonferroni Corrected P-Values (Spread)\")\n",
    "    print(p_values_corrected_df_spread_bonf.astype(float).round(4))\n",
    "    print(\"\\nFDR Corrected P-Values (Spread)\")\n",
    "    print(p_values_corrected_df_spread_fdr.astype(float).round(4))\n",
    "\n",
    "\n",
    "    # Choose correction method for heatmap\n",
    "    if CORRECTION_METHOD == 'Bonferroni':\n",
    "        sig_matrix_spread = p_values_corrected_df_spread_bonf\n",
    "        sig_matrix_volume = p_values_corrected_df_volume_bonf\n",
    "    else:\n",
    "        sig_matrix_spread = p_values_corrected_df_spread_fdr\n",
    "        sig_matrix_volume = p_values_corrected_df_volume_fdr\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7)) # Increased width slightly for more columns\n",
    "    sns.heatmap(p_values_spread_df, annot=True, cmap='viridis_r', fmt=\".3f\", vmin=0, vmax=1,\n",
    "                linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (uncorrected)'})\n",
    "    for i in range(len(sig_matrix_spread.index)):\n",
    "        for j in range(len(sig_matrix_spread.columns)):\n",
    "            val = sig_matrix_spread.iloc[i, j]\n",
    "            if pd.notnull(val) and val < 0.05:\n",
    "                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, \n",
    "                                        edgecolor='red', lw=3))\n",
    "    ax.set_title(\n",
    "        f'P-values for Spread (vs. Control) | {SPREAD_METHOD_NAME}\\n\\nRed border = Significant ({CORRECTION_METHOD_NAME} < 0.05)', \n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "    # Heatmaps usually don't need explicit X/Y labels if the ticks are descriptive,\n",
    "    # but adding them for clarity.\n",
    "    ax.set_xlabel('Holiday Events', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Market Cap Group', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/Figure_3A_PValues_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7)) # Increased width slightly\n",
    "    sns.heatmap(p_values_volume_df, annot=True, cmap='plasma_r', fmt=\".3f\", vmin=0, vmax=1, \n",
    "                linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (uncorrected)'})\n",
    "    for i in range(len(sig_matrix_volume.index)):\n",
    "        for j in range(len(sig_matrix_volume.columns)):\n",
    "            val = sig_matrix_volume.iloc[i, j]\n",
    "            if pd.notnull(val) and val < 0.05:\n",
    "                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, \n",
    "                                        edgecolor='red', lw=3))\n",
    "    ax.set_title(\n",
    "        f'P-values for Volume (vs. Control) | {SPREAD_METHOD_NAME}\\nRed border = Significant ({CORRECTION_METHOD_NAME} < 0.05)', \n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "    ax.set_xlabel('Holiday Events', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Market Cap Group', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/Figure_3B_PValues_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 4: HEATMAP - SPREAD BY CAP AND PERIOD\n",
    "period_order = ['control_year', 'pre_easter', 'post_easter',\n",
    "                'pre_christmas', 'christmas', 'post_christmas',\n",
    "                'summer_excl_holiday', 'summer_holiday']\n",
    "period_order = [p for p in period_order if p in df['period_label'].unique()]\n",
    "\n",
    "\n",
    "heatmap_data = df.groupby(['Cap_Group', 'period_label'])['Spread'].mean().unstack()\n",
    "heatmap_data = heatmap_data[period_order]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.heatmap(heatmap_data, \n",
    "            annot=True, \n",
    "            fmt='.5f', \n",
    "            cmap='YlOrRd', \n",
    "            linewidths=0.5, \n",
    "            cbar_kws={'label': 'Avg Spread'},\n",
    "            ax=ax,\n",
    "            annot_kws={'fontsize': 11})\n",
    "ax.set_title(f'Average Spread by Cap Group and Period | {SPREAD_METHOD_NAME}', \n",
    "             fontweight='bold', \n",
    "             fontsize=18, \n",
    "             pad=15)\n",
    "ax.set_xlabel('Period', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Market Cap Group', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='y', labelsize=13)\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Figure_4_Heatmap_Spread.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 5: TIME SERIES - MONTHLY AVERAGE SPREAD\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "monthly_spread = df.groupby('YearMonth')['Spread'].mean()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(monthly_spread.index.to_timestamp(), monthly_spread.values, \n",
    "        color='darkblue', linewidth=1.5, alpha=0.8)\n",
    "ax.set_title(f'Monthly Average Spread (2014-2019) | {SPREAD_METHOD_NAME} (In-Sample)', fontweight='bold', fontsize=16)\n",
    "ax.set_xlabel('Date (Year-Month)', fontsize=14)\n",
    "ax.set_ylabel('Average Relative Spread', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Figure_5_Timeseries_Monthly.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 6: VOLUME-SPREAD RELATIONSHIP (HARRIS VALIDATION)\n",
    "# Sample to avoid overplotting (take 10% random sample)\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "\n",
    "df_sample_clean = df_sample.dropna(subset=['Volume', 'Spread'])\n",
    "df_sample_clean = df_sample_clean[~np.isinf(df_sample_clean['Volume'])]\n",
    "df_sample_clean = df_sample_clean[~np.isinf(df_sample_clean['Spread'])]\n",
    "df_sample_clean = df_sample_clean[df_sample_clean['Spread'] <= 0.20]\n",
    "df_sample_clean = df_sample_clean[df_sample_clean['Volume'] > 0]\n",
    "\n",
    "\n",
    "print(f\"Sample size after cleaning: {len(df_sample_clean):,} observations\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "\n",
    "# Scatter plot with color by cap group\n",
    "for cap in ['Large Cap', 'Mid Cap', 'Small Cap']:\n",
    "    data = df_sample_clean[df_sample_clean['Cap_Group'] == cap]\n",
    "    ax.scatter(np.log10(data['Volume']), data['Spread'], \n",
    "               alpha=0.3, s=10, label=cap)\n",
    "\n",
    "\n",
    "# Add regression line\n",
    "log_vol = np.log10(df_sample_clean['Volume'])\n",
    "slope, intercept, r_value, p_value, std_err = linregress(log_vol, df_sample_clean['Spread'])\n",
    "\n",
    "\n",
    "# Only plot trend line if r_value is valid\n",
    "if not np.isnan(r_value):\n",
    "    ax.plot(log_vol.sort_values(), intercept + slope*log_vol.sort_values(), 'r--', linewidth=2, \n",
    "            label=f'Trend (R²={r_value**2:.3f}, p={p_value:.4f})')\n",
    "else:\n",
    "    print(\"Warning: Could not compute valid regression line\")\n",
    "\n",
    "\n",
    "ax.set_xlabel('Log Trading Volume (Activity)', fontsize=14)\n",
    "ax.set_ylabel('Relative Spread (Cost)', fontsize=14)\n",
    "ax.set_title(f'Volume-Spread Relationship | Harris Dealer Hypothesis\\n{SPREAD_METHOD_NAME}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Figure_6_Volume_Spread_Scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# FIGURE 7: SEASONALITY (VOLUME vs VOLATILITY)\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "# Calculate Daily Returns if not already present\n",
    "df['Daily_Return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "\n",
    "# Calculate Volatility (Std Dev of Daily Returns within that month)\n",
    "monthly_stats = df.groupby('Month').agg({\n",
    "    'Volume': 'mean',\n",
    "    'Daily_Return': lambda x: x.std() * 100\n",
    "}).reset_index()\n",
    "\n",
    "monthly_stats.columns = ['Month', 'Avg_Volume', 'Avg_Volatility']\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "color_vol = '#1e3a8a'\n",
    "color_volat = '#15803d'\n",
    "\n",
    "plt.suptitle('Seasonality of Volume vs. Volatility (Average 2014-2019)', fontsize=19, fontweight='bold', y=1.02)\n",
    "ax1.set_title('Monthly Pattern with July (Summer) Highlighted', fontsize=12, loc='left', fontweight='bold', pad=20)\n",
    "\n",
    "\n",
    "month_positions = monthly_stats['Month']\n",
    "bars = ax1.bar(\n",
    "    month_positions, monthly_stats['Avg_Volume'],\n",
    "    color=color_vol, alpha=0.9, label='Avg Daily Volume', edgecolor='black', linewidth=1, width=0.75\n",
    ")\n",
    "\n",
    "ylim_top = 900000\n",
    "ax1.set_ylim(0, ylim_top)\n",
    "summer_patch = patches.Rectangle((6.5, 0), 1, ylim_top, linewidth=0, facecolor='#fef3c7', alpha=0.6, zorder=0)\n",
    "ax1.add_patch(summer_patch)\n",
    "ax1.text(7, ylim_top*0.96, 'Summer (July)', ha='center', fontsize=13, fontweight='bold', color='#92400e',\n",
    "         bbox=dict(facecolor='white', edgecolor='#92400e', alpha=1, pad=5, linewidth=2))\n",
    "\n",
    "ax1.set_xlabel('Month (Jan-Dec)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Avg Daily Volume', color=color_vol, fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='y', labelcolor=color_vol, labelsize=12)\n",
    "ax1.set_xticks(range(1, 13))\n",
    "ax1.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.25, linestyle='--', linewidth=1)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line = ax2.plot(\n",
    "    month_positions, monthly_stats['Avg_Volatility'],\n",
    "    color=color_volat, linewidth=4, marker='o', markersize=11, label='Volatility %',\n",
    "    markerfacecolor=color_volat, markeredgecolor='white', markeredgewidth=2.5, zorder=10\n",
    ")\n",
    "ax2.set_ylabel('Volatility % (Std Dev of Returns)', color=color_volat, fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='y', labelcolor=color_volat, labelsize=12)\n",
    "ax2.set_ylim(0, 15)\n",
    "\n",
    "highlight_months = [1, 7, 12]\n",
    "for i, (x, y) in enumerate(zip(month_positions, monthly_stats['Avg_Volatility'])):\n",
    "    if i+1 in highlight_months:\n",
    "        ax2.annotate(f\"{y:.2f}%\", (x, y), textcoords=\"offset points\", xytext=(0, 15), ha='center', \n",
    "                     fontsize=12, fontweight='bold',\n",
    "                     bbox=dict(facecolor='white', edgecolor=color_volat, alpha=1, pad=4, boxstyle='round,pad=0.5', linewidth=2.5), \n",
    "                     color=color_volat, zorder=20)\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "leg = ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', frameon=True, fontsize=13, \n",
    "                 framealpha=1, edgecolor='black', fancybox=False, shadow=False)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.savefig('output/Figure_7_Seasonality_Vol_Volat.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nAll outputs saved to 'output/' folder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "945f6bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIF MULTICOLLINEARITY DIAGNOSTICS\n",
      "Variance Inflation Factors:\n",
      "       Variable      VIF\n",
      "      Large_Cap 1.771136\n",
      "        Mid_Cap 1.509109\n",
      "Log_Volume_lag1 1.236974\n",
      "     Volatility 1.029921\n",
      "         Summer 1.009086\n",
      "      Christmas 1.006896\n",
      "  Pre_Christmas 1.003932\n",
      "     Pre_Easter 1.003596\n",
      "    Post_Easter 1.003423\n",
      " Post_Christmas 1.003360\n",
      "Model 3: R-squared = 0.2488, N = 53300\n",
      "\n",
      "Key Coefficients:\n",
      "                 Coefficient  Std Error     P>|t|\n",
      "const              -0.011581   0.004259  0.006547\n",
      "Pre_Easter         -0.000164   0.000314  0.601824\n",
      "Post_Easter         0.000367   0.000383  0.337583\n",
      "Christmas           0.001934   0.000420  0.000004\n",
      "Pre_Christmas       0.001194   0.000611  0.050827\n",
      "Post_Christmas      0.002346   0.000507  0.000004\n",
      "Summer             -0.001369   0.000303  0.000006\n",
      "Log_Volume_lag1     0.001621   0.000376  0.000016\n",
      "Volatility          0.054230   0.027748  0.050657\n",
      "SUCCESS: Saved model summary to output/Table_3_Stock_Fixed_Effects.png\n",
      "Model 4: R-squared (within) = 0.0416, N = 53300\n",
      "\n",
      "Panel Regression Coefficients:\n",
      "                                Parameter Estimates                                \n",
      "===================================================================================\n",
      "                 Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------\n",
      "Pre_Easter         -0.0002     0.0003    -0.5257     0.5991     -0.0008      0.0004\n",
      "Post_Easter         0.0004     0.0004     0.9661     0.3340     -0.0004      0.0011\n",
      "Christmas           0.0019     0.0004     4.6413     0.0000      0.0011      0.0028\n",
      "Pre_Christmas       0.0012     0.0006     1.9675     0.0491    4.52e-06      0.0024\n",
      "Post_Christmas      0.0023     0.0005     4.6614     0.0000      0.0014      0.0033\n",
      "Summer             -0.0014     0.0003    -4.5561     0.0000     -0.0020     -0.0008\n",
      "Log_Volume_lag1     0.0016     0.0004     4.3470     0.0000      0.0009      0.0024\n",
      "Volatility          0.0542     0.0275     1.9689     0.0490      0.0002      0.1082\n",
      "===================================================================================\n",
      "\n",
      "--- MODEL COMPARISON ---\n",
      "             Model  Summer Coef  Summer P-val  R-squared\n",
      "     Model 2 (OLS)    -0.001554      0.000001   0.125596\n",
      "Model 3 (Stock FE)    -0.001369      0.000006   0.248784\n",
      "Model 4 (Panel FE)    -0.001369      0.000005   0.041558\n",
      "All regression tables have been saved as PNG images in the 'output' folder.\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 3: OLS REGRESSION ANALYSIS WITH DYNAMIC DESCRIPTIVE TITLES\n",
    "INPUT_CSV = \"oslo_bors_labelled_data.csv\"\n",
    "# Use the variable SPREAD_METHOD_NAME if defined, else default\n",
    "spread_name = SPREAD_METHOD_NAME if 'SPREAD_METHOD_NAME' in locals() else \"Spread\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = 'output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# DATA LOADING & INTEGRITY CHECK\n",
    "# Use 'df' from Script 2 (In-Sample 2014-2019) if available to ensure consistency.\n",
    "if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
    "    # Double check to ensure no look-ahead bias\n",
    "    max_year = df['Date'].dt.year.max()\n",
    "    if max_year > 2019:\n",
    "        print(f\"WARNING: Data contains years up to {max_year}. Applying 2019 cutoff now.\")\n",
    "        df = df[df['Date'].dt.year <= 2019].copy()\n",
    "else:\n",
    "    print(\"Variable 'df' not found. Loading from CSV and applying split-sample...\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV, parse_dates=['Date'])\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        # Apply In-Sample Filter\n",
    "        df = df[df['Date'].dt.year <= 2019].copy()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: '{INPUT_CSV}' not found.\")\n",
    "        pass\n",
    "\n",
    "# Data Preparation\n",
    "df = df.sort_values(['Ticker', 'Date'])\n",
    "df['Return'] = df.groupby('Ticker')['Close'].pct_change()\n",
    "df['Volatility'] = df.groupby('Ticker')['Return'].transform(lambda x: x.shift(1).rolling(window=20, min_periods=10).std())\n",
    "df['Log_Volume'] = np.log1p(df['Volume'])\n",
    "df['Log_Volume_lag1'] = df.groupby('Ticker')['Log_Volume'].shift(1)\n",
    "\n",
    "# Holiday and Market Cap Dummies\n",
    "df['Pre_Easter'] = (df['period_label'] == 'pre_easter').astype(int)\n",
    "df['Post_Easter'] = (df['period_label'] == 'post_easter').astype(int)\n",
    "df['Christmas'] = (df['period_label'] == 'christmas').astype(int)\n",
    "df['Pre_Christmas'] = (df['period_label'] == 'pre_christmas').astype(int)\n",
    "df['Post_Christmas'] = (df['period_label'] == 'post_christmas').astype(int)\n",
    "df['Summer'] = (df['period_label'] == 'summer_holiday').astype(int)\n",
    "df['Large_Cap'] = (df['Cap_Group'] == 'Large Cap').astype(int)\n",
    "df['Mid_Cap'] = (df['Cap_Group'] == 'Mid Cap').astype(int)\n",
    "\n",
    "# Create regression sample\n",
    "df_reg = df.dropna(subset=['Spread', 'Log_Volume_lag1', 'Volatility']).copy()\n",
    "\n",
    "# VIF MULTICOLLINEARITY CHECK\n",
    "print(\"\\nVIF MULTICOLLINEARITY DIAGNOSTICS\")\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor (Already imported in Script 0)\n",
    "\n",
    "vif_vars = ['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', \n",
    "            'Summer', 'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']\n",
    "X_vif = df_reg[vif_vars].copy()\n",
    "X_vif = sm.add_constant(X_vif) # Add constant for correct VIF calculation\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "vif_data = vif_data[vif_data[\"Variable\"] != \"const\"].sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"Variance Inflation Factors:\")\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(\"\\nWARNING: High multicollinearity detected (VIF > 10)\")\n",
    "    print(high_vif.to_string(index=False))\n",
    "\n",
    "# Models 1A-C: Baseline Holiday Effects by Market Cap\n",
    "baseline_models = {}\n",
    "cap_groups = ['Large Cap', 'Mid Cap', 'Small Cap']\n",
    "\n",
    "for i, cap in enumerate(cap_groups):\n",
    "    df_cap = df_reg[df_reg['Cap_Group'] == cap].copy()\n",
    "    \n",
    "    y = df_cap['Spread']\n",
    "    X = df_cap[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer']]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Using Cluster Robust Standard Errors (HC1 equivalent with groups)\n",
    "    # Note: statsmodels OLS cov_type='cluster' requires 'cov_kwds={'groups': ...}'\n",
    "    model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': df_cap['Ticker']})\n",
    "    baseline_models[cap] = model\n",
    "    \n",
    "    # Create and save summary as an image with the dynamic title (v10 style)\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.axis('off')\n",
    "    title_text = f\"Baseline Spread Regression - {cap} Stocks\\nDependent Variable: {spread_name}\"\n",
    "    ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "    summary_text = str(model.summary())\n",
    "    ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "    \n",
    "    filename = f\"output/Table_1{chr(65+i)}_Baseline_{cap.replace(' ', '_')}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Model 2: Full Sample with Controls\n",
    "X2 = df_reg[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer',\n",
    "             'Log_Volume_lag1', 'Volatility', 'Large_Cap', 'Mid_Cap']]\n",
    "X2 = sm.add_constant(X2)\n",
    "y2 = df_reg['Spread']\n",
    "\n",
    "# Clustered SE\n",
    "model2 = sm.OLS(y2, X2).fit(cov_type='cluster', cov_kwds={'groups': df_reg['Ticker']})\n",
    "\n",
    "# Save Model 2\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = (f'Spread Determinants with Control Variables (Full Sample)\\n'\n",
    "              f'Dependent Variable: {spread_name} | Controls: Lagged Volume, Volatility, Market Cap')\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model2.summary())\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_2 = 'output/Table_2_Spread_Full_Controls.png'\n",
    "plt.savefig(filename_2, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# MODEL 3: STOCK FIXED EFFECTS\n",
    "# Create dummies explicitly for reporting count\n",
    "stock_dummies = pd.get_dummies(df_reg['Ticker'], prefix='Stock', drop_first=True, dtype=float)\n",
    "\n",
    "X3 = pd.concat([\n",
    "    df_reg[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', 'Post_Christmas', 'Summer',\n",
    "            'Log_Volume_lag1', 'Volatility']].astype(float),\n",
    "    stock_dummies\n",
    "], axis=1)\n",
    "X3 = sm.add_constant(X3)\n",
    "\n",
    "# Note: This might be slow with many stocks. \n",
    "# If memory error, use linearmodels (Model 4) which is optimized for FE.\n",
    "# But keeping OLS here as requested to match v10 output structure.\n",
    "model3 = sm.OLS(df_reg['Spread'].astype(float), X3).fit(cov_type='cluster', cov_kwds={'groups': df_reg['Ticker']})\n",
    "\n",
    "print(f\"Model 3: R-squared = {model3.rsquared:.4f}, N = {model3.nobs:.0f}\")\n",
    "\n",
    "# Key coefficients table\n",
    "coef_interest = ['const', 'Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', \n",
    "                 'Post_Christmas', 'Summer', 'Log_Volume_lag1', 'Volatility']\n",
    "coef_table = pd.DataFrame({\n",
    "    'Coefficient': model3.params[coef_interest],\n",
    "    'Std Error': model3.bse[coef_interest],\n",
    "    'P>|t|': model3.pvalues[coef_interest]\n",
    "}).round(6)\n",
    "\n",
    "print(\"\\nKey Coefficients:\")\n",
    "print(coef_table.to_string())\n",
    "\n",
    "# Save Model 3 (v10 style)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.axis('off')\n",
    "title_text = f'Spread Regression with Stock Fixed Effects (In-Sample)\\nDependent Variable: {spread_name}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = f\"\"\"OLS Regression Results (with Stock FE)\n",
    "R-squared: {model3.rsquared:.4f}\n",
    "Observations: {model3.nobs:.0f}\n",
    "Stock FE: {len(stock_dummies.columns)} dummies included\n",
    "\n",
    "{coef_table.to_string()}\n",
    "\n",
    "Notes: Standard errors clustered by Ticker\n",
    "\"\"\"\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 9}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_3 = 'output/Table_3_Stock_Fixed_Effects.png'\n",
    "plt.savefig(filename_3, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"SUCCESS: Saved model summary to {filename_3}\")\n",
    "\n",
    "# MODEL 4: PANEL REGRESSION\n",
    "df_panel = df_reg.set_index(['Ticker', 'Date']).copy()\n",
    "y_panel = df_panel['Spread']\n",
    "X_panel = df_panel[['Pre_Easter', 'Post_Easter', 'Christmas', 'Pre_Christmas', \n",
    "                    'Post_Christmas', 'Summer', 'Log_Volume_lag1', 'Volatility']]\n",
    "\n",
    "# IMPORTANT: Using Entity AND Time Effects as in Table 4 v10\n",
    "model4 = PanelOLS(y_panel, X_panel, entity_effects=True, time_effects=False).fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "print(f\"Model 4: R-squared (within) = {model4.rsquared:.4f}, N = {model4.nobs:.0f}\")\n",
    "print(\"\\nPanel Regression Coefficients:\")\n",
    "print(model4.summary.tables[1])\n",
    "\n",
    "# Save Model 4 (v10 style)\n",
    "fig, ax = plt.subplots(figsize=(12, 11))\n",
    "ax.axis('off')\n",
    "title_text = f'Panel Regression with Entity Fixed Effects (In-Sample)\\nDependent Variable: {spread_name}'\n",
    "ax.text(0.5, 0.98, title_text, ha='center', va='top', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
    "summary_text = str(model4.summary)\n",
    "ax.text(0.01, 0.93, summary_text, fontdict={'fontname': 'monospace', 'fontsize': 8}, va='top', ha='left', transform=ax.transAxes)\n",
    "filename_4 = 'output/Table_4_Panel_Entity_Time_FE.png'\n",
    "plt.savefig(filename_4, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# MODEL COMPARISON\n",
    "print(\"\\n--- MODEL COMPARISON ---\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Model 2 (OLS)', 'Model 3 (Stock FE)', 'Model 4 (Panel FE)'],\n",
    "    'Summer Coef': [model2.params.get('Summer', np.nan), model3.params.get('Summer', np.nan), model4.params.get('Summer', np.nan)],\n",
    "    'Summer P-val': [model2.pvalues.get('Summer', np.nan), model3.pvalues.get('Summer', np.nan), model4.pvalues.get('Summer', np.nan)],\n",
    "    'R-squared': [model2.rsquared, model3.rsquared, model4.rsquared]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "comparison_df.to_csv('output/Model_Comparison_Summary.csv', index=False)\n",
    "\n",
    "print(\"All regression tables have been saved as PNG images in the 'output' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef62776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 54,135 observations (2014-2019)\n",
      "CHECK 1: IS THE EFFECT CONSISTENT ACROSS YEARS? (INTERACTION ANALYSIS)\n",
      "Hypothesis: If the effect is structural, 'Summer' coefficient should be negative in most years.\n",
      "Year   Summer Coef  P-Value    Signif    \n",
      "---------------------------------------------\n",
      "2014   -0.002423    0.0000     ***       \n",
      "2015   -0.002078    0.0013     ***       \n",
      "2016   -0.003038    0.0000     ***       \n",
      "2017   -0.001298    0.0282     **        \n",
      "2018   -0.001665    0.0004     ***       \n",
      "2019   -0.002941    0.0000     ***       \n",
      "CHECK 2: PLACEBO TEST (RANDOM PERIODS)\n",
      "Testing 100 random 'fake' summer periods. Real effect should be stronger (more negative).\n",
      "\n",
      "Average Placebo Coefficient: -0.000000\n",
      "Actual Average Summer Coefficient: -0.002240\n",
      "Probability that random noise generates this effect: 0.0000\n",
      "Robustness checks completed and figures saved in output folder.\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 4: IN-SAMPLE ROBUSTNESS & STABILITY ANALYSIS (2014-2019)\n",
    "\n",
    "# Ensure we are using the correct In-Sample data\n",
    "if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
    "    # Verify it's In-Sample\n",
    "    max_year = df['Date'].dt.year.max()\n",
    "    if max_year > 2019:\n",
    "        print(f\"Data includes {max_year}. Trimming to 2014-2019 for robustness checks.\")\n",
    "        df_robust = df[df['Date'].dt.year <= 2019].copy()\n",
    "    else:\n",
    "        df_robust = df.copy()\n",
    "else:\n",
    "    # Fallback loading\n",
    "    df_robust = pd.read_csv('oslo_bors_labelled_data.csv', parse_dates=['Date'])\n",
    "    df_robust['Date'] = pd.to_datetime(df_robust['Date'])\n",
    "    df_robust = df_robust[df_robust['Date'].dt.year <= 2019].copy()\n",
    "\n",
    "# Prepare variables if not present (safety check)\n",
    "if 'Summer' not in df_robust.columns:\n",
    "    df_robust['Summer'] = (df_robust['period_label'] == 'summer_holiday').astype(int)\n",
    "if 'Log_Volume' not in df_robust.columns:\n",
    "    df_robust['Log_Volume'] = np.log1p(df_robust['Volume'])\n",
    "\n",
    "print(f\"Data ready: {len(df_robust):,} observations (2014-2019)\")\n",
    "\n",
    "# ROBUSTNESS CHECK 1: YEAR-BY-YEAR STABILITY\n",
    "print(\"CHECK 1: IS THE EFFECT CONSISTENT ACROSS YEARS? (INTERACTION ANALYSIS)\")\n",
    "print(\"Hypothesis: If the effect is structural, 'Summer' coefficient should be negative in most years.\")\n",
    "\n",
    "# We create dummy variables for each year and interact them with Summer\n",
    "# Formula: Spread ~ Summer * Year_Dummies + Controls\n",
    "# This gives us the \"Summer Effect\" specific to each year.\n",
    "\n",
    "years = sorted(df_robust['Date'].dt.year.unique())\n",
    "results_by_year = []\n",
    "\n",
    "print(f\"{'Year':<6} {'Summer Coef':<12} {'P-Value':<10} {'Signif':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for year in years:\n",
    "    # Subset for the specific year\n",
    "    df_y = df_robust[df_robust['Date'].dt.year == year].dropna(subset=['Spread', 'Log_Volume'])\n",
    "    \n",
    "    if len(df_y) < 100: continue\n",
    "    \n",
    "    # Simple regression for that year: Spread ~ Summer + Log_Volume (Control)\n",
    "    X = df_y[['Summer', 'Log_Volume']]\n",
    "    X = sm.add_constant(X)\n",
    "    y = df_y['Spread']\n",
    "    \n",
    "    try:\n",
    "        model_y = sm.OLS(y, X).fit(cov_type='HC1')\n",
    "        coef = model_y.params['Summer']\n",
    "        pval = model_y.pvalues['Summer']\n",
    "        \n",
    "        sig = \"\"\n",
    "        if pval < 0.01: sig = \"***\"\n",
    "        elif pval < 0.05: sig = \"**\"\n",
    "        elif pval < 0.1: sig = \"*\"\n",
    "        \n",
    "        results_by_year.append({\n",
    "            'Year': year,\n",
    "            'Coef': coef,\n",
    "            'Lower_CI': model_y.conf_int().loc['Summer', 0],\n",
    "            'Upper_CI': model_y.conf_int().loc['Summer', 1],\n",
    "            'P_Value': pval\n",
    "        })\n",
    "        \n",
    "        print(f\"{year:<6} {coef:<12.6f} {pval:<10.4f} {sig:<10}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{year:<6} ERROR: {str(e)}\")\n",
    "\n",
    "# Plotting the Stability\n",
    "res_df = pd.DataFrame(results_by_year)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.errorbar(res_df['Year'], res_df['Coef'], \n",
    "            yerr=[res_df['Coef'] - res_df['Lower_CI'], res_df['Upper_CI'] - res_df['Coef']], \n",
    "            fmt='o-', color='darkblue', capsize=5, label='Summer Effect (95% CI)')\n",
    "\n",
    "ax.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "ax.set_title('Stability Check Summer Effect Year-by-Year (In-Sample)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Coefficient Estimate (Summer Effect)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig('output/Figure_8_Robustness_Yearly_Stability.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ROBUSTNESS CHECK 2: PLACEBO TEST (RANDOM WEEKS)\n",
    "print(\"CHECK 2: PLACEBO TEST (RANDOM PERIODS)\")\n",
    "print(\"Testing 100 random 'fake' summer periods. Real effect should be stronger (more negative).\")\n",
    "\n",
    "n_simulations = 100\n",
    "placebo_coefs = []\n",
    "\n",
    "# Filter to non-holiday data for placebo testing to avoid contamination\n",
    "df_clean = df_robust[df_robust['period_label'] == 'control_year'].copy()\n",
    "sample_size = len(df_robust[df_robust['Summer'] == 1]) # Match size of actual summer\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # Assign random \"Summer\" dummy\n",
    "    # We randomly select 4 consecutive weeks in the dataset to call \"Fake Summer\"\n",
    "    # Simplified: Randomly shuffle the 'Summer' tag (Permutation Test)\n",
    "    \n",
    "    # Faster approach: Permutation of the variable\n",
    "    fake_summer = np.random.permutation(df_robust['Summer'].values)\n",
    "    \n",
    "    # Quick regression (using numpy for speed)\n",
    "    # y = Xb. Spread ~ const + fake_summer\n",
    "    y = df_robust['Spread'].values\n",
    "    x = fake_summer\n",
    "    # Simple difference in means is equivalent to reg coef on dummy\n",
    "    mean_fake_summer = y[x == 1].mean()\n",
    "    mean_control = y[x == 0].mean()\n",
    "    coef = mean_fake_summer - mean_control\n",
    "    placebo_coefs.append(coef)\n",
    "\n",
    "actual_summer_coef = res_df['Coef'].mean() # Average effect\n",
    "\n",
    "# Calculate p-value from permutation\n",
    "placebo_coefs = np.array(placebo_coefs)\n",
    "p_perm = (placebo_coefs < actual_summer_coef).mean()\n",
    "\n",
    "print(f\"\\nAverage Placebo Coefficient: {placebo_coefs.mean():.6f}\")\n",
    "print(f\"Actual Average Summer Coefficient: {actual_summer_coef:.6f}\")\n",
    "print(f\"Probability that random noise generates this effect: {p_perm:.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(placebo_coefs, kde=True, color='gray', ax=ax, label='Placebo Distribution')\n",
    "ax.axvline(actual_summer_coef, color='red', linestyle='--', linewidth=2, label='Actual Summer Effect')\n",
    "ax.set_title('Robustness Placebo Test vs Actual Effect (In-Sample)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency (Count)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.savefig('output/Figure_9_Robustness_Placebo.png', dpi=300)\n",
    "plt.close()\n",
    "print('Robustness checks completed and figures saved in output folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7939c8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Execution Quality (Effective Spread vs Quoted Spread)\n",
      "Theory: Harris (2003) - The Trader's Dilemma (Limit vs. Market Orders)\n",
      "   Universe: ['NHY.OL', 'DNO.OL', 'NEL.OL', 'EQNR.OL', 'RECSI.OL']\n",
      "   Data Points: 3,178 rows (2020-2024)\n",
      "\n",
      "FINAL REPORT: EXECUTION STRATEGY ANALYSIS (HARRIS THEORY)\n",
      "--------------------------------------------------------------\n",
      "1. THEORETICAL FRAMEWORK\n",
      "   - Tested \"Trader's Dilemma\": Paying for Immediacy (Market Orders) \n",
      "     vs. Risking Non-Execution for Price Improvement (Limit Orders).\n",
      "   - Focus: Reducing Transaction Costs (TCA) in the Summer Period.\n",
      "\n",
      "2. RESULTS (2020-2024)\n",
      "   - Average Cost (Aggressive): 64.7 bps\n",
      "   - Average Cost (Passive):    45.8 bps\n",
      "   - Net Improvement (Alpha):   18.9 bps\n",
      "\n",
      "3. ECONOMIC SIGNIFICANCE\n",
      "   - By using Passive Limit Orders instead of Aggressive Market Orders during\n",
      "     the summer, the investor captured a \"Liquidity Premium\".\n",
      "   - Total Estimated Savings (spread capture): 9,441 NOK\n",
      "\n",
      "Final execution strategy report saved to 'output/Final_Execution_Report.txt'\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 5: MICROSTRUCTURE EXECUTION BACKTEST (LIMIT VS MARKET)\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12, \n",
    "    'figure.dpi': 300,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12\n",
    "})\n",
    "\n",
    "print(\"Compare Execution Quality (Effective Spread vs Quoted Spread)\")\n",
    "print(\"Theory: Harris (2003) - The Trader's Dilemma (Limit vs. Market Orders)\")\n",
    "\n",
    "# DATA LOADING & PREPARATION\n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv('oslo_bors_labelled_data.csv', parse_dates=['Date'])\n",
    "except FileNotFoundError:\n",
    "    print(\"CRITICAL ERROR: 'oslo_bors_labelled_data.csv' not found.\")\n",
    "    raise\n",
    "\n",
    "df_full['Year'] = df_full['Date'].dt.year\n",
    "df_full = df_full.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Filter: Out-of-Sample (2020-2024)\n",
    "df_in_sample = df_full[df_full['Year'] <= 2019]\n",
    "top_5_tickers = df_in_sample.groupby('Ticker')['Volume'].median().nlargest(5).index.tolist()\n",
    "\n",
    "df_oos = df_full[(df_full['Year'] >= 2020) & (df_full['Ticker'].isin(top_5_tickers))].copy()\n",
    "\n",
    "print(f\"   Universe: {top_5_tickers}\")\n",
    "print(f\"   Data Points: {len(df_oos):,} rows (2020-2024)\")\n",
    "\n",
    "# EXECUTION SIMULATION ENGINE\n",
    "\n",
    "PORTFOLIO_CAPITAL = 1000000  # 1 Million NOK\n",
    "ALLOCATION = PORTFOLIO_CAPITAL / 5\n",
    "LIMIT_FILL_PROB = 0.60  # 60% Fill Rate at Midpoint\n",
    "\n",
    "results = []\n",
    "years = sorted(df_oos['Year'].unique())\n",
    "\n",
    "for year in years:\n",
    "    df_year = df_oos[df_oos['Year'] == year]\n",
    "    \n",
    "    # Week 29 Strategy\n",
    "    summer_weeks = df_year[df_year['Date'].dt.isocalendar().week == 29]\n",
    "    if summer_weeks.empty:\n",
    "        trade_date = df_year[df_year['Date'].dt.isocalendar().week == 30]['Date'].min()\n",
    "    else:\n",
    "        trade_date = summer_weeks['Date'].min()\n",
    "        \n",
    "    if pd.isna(trade_date): continue\n",
    "    \n",
    "    cost_aggressive_market = 0\n",
    "    cost_passive_limit = 0\n",
    "    savings_spread = 0\n",
    "    \n",
    "    for ticker in top_5_tickers:\n",
    "        stock_data = df_oos[df_oos['Ticker'] == ticker].sort_values('Date')\n",
    "        \n",
    "        try:\n",
    "            idx = stock_data[stock_data['Date'] == trade_date].index[0]\n",
    "            loc = stock_data.index.get_loc(idx)\n",
    "            \n",
    "            if loc == 0: continue\n",
    "            \n",
    "            row_prev = stock_data.iloc[loc - 1]\n",
    "            row_curr = stock_data.iloc[loc]\n",
    "            \n",
    "            P_benchmark = row_prev['Close']\n",
    "            P_market_close = row_curr['Close']\n",
    "            Spread_pct = row_curr['Spread']\n",
    "            \n",
    "            shares = ALLOCATION / P_benchmark\n",
    "            \n",
    "            half_spread_val = (Spread_pct * P_market_close / 2) * shares\n",
    "            price_drift_val = (P_market_close - P_benchmark) * shares\n",
    "            \n",
    "            # Strategy A: Market Order\n",
    "            total_cost_market = price_drift_val + half_spread_val\n",
    "            cost_aggressive_market += total_cost_market\n",
    "            \n",
    "            # Strategy B: Limit Order\n",
    "            expected_spread_paid = (1 - LIMIT_FILL_PROB) * half_spread_val\n",
    "            total_cost_limit = price_drift_val + expected_spread_paid\n",
    "            \n",
    "            cost_passive_limit += total_cost_limit\n",
    "            savings_spread += (half_spread_val - expected_spread_paid)\n",
    "            \n",
    "        except (IndexError, KeyError):\n",
    "            continue\n",
    "            \n",
    "    results.append({\n",
    "        'Year': year,\n",
    "        'Aggressive_IS_bps': (cost_aggressive_market / PORTFOLIO_CAPITAL) * 10000,\n",
    "        'Passive_IS_bps': (cost_passive_limit / PORTFOLIO_CAPITAL) * 10000,\n",
    "        'Spread_Savings_NOK': savings_spread\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "df_res['Net_Improvement_bps'] = df_res['Aggressive_IS_bps'] - df_res['Passive_IS_bps']\n",
    "\n",
    "# VISUALIZATION (FIGURE 10)\n",
    "# FIGURE 10: JOINT HOLIDAY (WEEKS 28-30) VS REST OF YEAR (2020-2024)\n",
    "# Prepare Data (Same logic as before)\n",
    "df_fig10 = df_oos.copy()\n",
    "df_fig10['Week'] = df_fig10['Date'].dt.isocalendar().week\n",
    "df_fig10['Season'] = np.where(df_fig10['Week'].isin([28, 29, 30]), \n",
    "                              'Joint Holiday (Week 28-30)', \n",
    "                              'Rest of Year')\n",
    "annual_stats = df_fig10.groupby(['Year', 'Season'])['Spread'].mean().unstack()\n",
    "annual_stats['Pct_Change'] = ((annual_stats['Joint Holiday (Week 28-30)'] - annual_stats['Rest of Year']) / annual_stats['Rest of Year']) * 100\n",
    "\n",
    "# Create Plot with 2 Panels\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [1.5, 1]})\n",
    "\n",
    "# PANEL A: ABSOLUTE SPREAD LEVELS\n",
    "x = np.arange(len(annual_stats.index))\n",
    "width = 0.35\n",
    "\n",
    "# Plot grouped bars\n",
    "rects1 = ax1.bar(x - width/2, annual_stats['Joint Holiday (Week 28-30)'] * 100, width, \n",
    "                 label='Joint Holiday', color='#e74c3c', alpha=0.9, edgecolor='black', linewidth=0.5)\n",
    "rects2 = ax1.bar(x + width/2, annual_stats['Rest of Year'] * 100, width, \n",
    "                 label='Rest of Year', color='#3498db', alpha=0.9, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_ylabel('Average Spread (%)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Panel A: Average Spread Levels Comparison', fontweight='bold', fontsize=14, loc='left')\n",
    "ax1.legend(loc='upper right', framealpha=0.95)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.set_ylim(0, 2.5)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax1.annotate(f'{height:.2f}%',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# PANEL B: RELATIVE COST CHANGE\n",
    "change_vals = annual_stats['Pct_Change']\n",
    "colors = ['#c0392b' if v > 0 else '#27ae60' for v in change_vals]\n",
    "\n",
    "bars = ax2.bar(x, change_vals, width=0.5, color=colors, alpha=0.85, edgecolor='black', linewidth=0.8)\n",
    "\n",
    "ax2.axhline(0, color='black', linewidth=1)\n",
    "ax2.set_ylabel('Relative Change (%)', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlabel('Year', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Panel B: Relative Cost Increase During Holiday (%)', fontweight='bold', fontsize=14, loc='left')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(annual_stats.index.astype(int), fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.set_xlim(-0.5, len(x) - 0.5)\n",
    "ax2.set_ylim(-35, 5)\n",
    "\n",
    "# Improved label placement for Panel B (avoiding overlap with axis)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    label_y = height + (1 if height >= 0 else -3) \n",
    "    \n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, label_y,\n",
    "             f'{height:+.1f}%',\n",
    "             ha='center', va='bottom' if height > 0 else 'top', \n",
    "             fontweight='bold', fontsize=11, color='#2c3e50')\n",
    "\n",
    "# Final Layout\n",
    "plt.suptitle('Liquidity Cost Analysis: Joint Holiday vs. Rest of Year', \n",
    "             fontsize=16, fontweight='bold', y=0.96)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90) \n",
    "\n",
    "plt.savefig('output/Figure_10_Joint_Holiday_vs_Rest.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FIGURE 11: BACKTEST RESULTS (CLEAN 2-PANEL LAYOUT)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True, gridspec_kw={'height_ratios': [1.5, 1]})\n",
    "\n",
    "# PANEL A: TRANSACTION COSTS (Comparison)\n",
    "years = df_res['Year']\n",
    "x = np.arange(len(years))\n",
    "width = 0.35\n",
    "\n",
    "# Plot bars\n",
    "rects1 = ax1.bar(x - width/2, df_res['Aggressive_IS_bps'], width, \n",
    "                 label='Aggressive Market Order', color='#e74c3c', alpha=0.9, edgecolor='black', linewidth=0.5)\n",
    "rects2 = ax1.bar(x + width/2, df_res['Passive_IS_bps'], width, \n",
    "                 label='Passive Limit Order', color='#27ae60', alpha=0.9, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Styling Panel A\n",
    "ax1.set_ylabel('Transaction Cost (bps)\\n(Neg = Price Improvement)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Panel A: Execution Cost Comparison (Limit vs. Market)', fontweight='bold', fontsize=14, loc='left')\n",
    "ax1.axhline(0, color='black', linewidth=1)\n",
    "ax1.legend(loc='upper left', framealpha=0.95)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars (optional, kept small)\n",
    "def autolabel(rects, ax):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        offset = 5 if height >= 0 else -15\n",
    "        ax.annotate(f'{height:.0f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, offset),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9, color='black')\n",
    "autolabel(rects1, ax1)\n",
    "autolabel(rects2, ax1)\n",
    "\n",
    "# PANEL B: NET SAVINGS (Liquidity Alpha)\n",
    "savings = df_res['Net_Improvement_bps']\n",
    "colors = ['#2980b9' if s >= 0 else '#c0392b' for s in savings]\n",
    "\n",
    "# Plot bars for savings\n",
    "bars = ax2.bar(x, savings, width=0.5, color=colors, alpha=0.8, edgecolor='black', linewidth=0.8)\n",
    "\n",
    "# Styling Panel B\n",
    "ax2.set_ylabel('Net Savings (bps)', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlabel('Year', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Panel B: \"Liquidity Alpha\" - Net Savings from Passive Strategy', fontweight='bold', fontsize=14, loc='left')\n",
    "ax2.axhline(0, color='black', linewidth=1)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(years.astype(int), fontsize=12)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.set_xlim(-0.5, len(x) - 0.5)\n",
    "\n",
    "# Add distinct labels for savings\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    label_y = height + (2 if height >= 0 else -5)\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, label_y, \n",
    "             f'+{height:.1f} bps', \n",
    "             ha='center', va='bottom', fontweight='bold', fontsize=11, color='#2c3e50')\n",
    "\n",
    "# Final layout adjustments\n",
    "plt.suptitle('Backtest Results: Week 29 Execution Strategy', fontsize=16, fontweight='bold', y=0.96)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "\n",
    "# Save\n",
    "plt.savefig('output/Figure_11_Backtest_Results.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "avg_aggressive = df_res['Aggressive_IS_bps'].mean()\n",
    "avg_passive = df_res['Passive_IS_bps'].mean()\n",
    "total_savings_nok = df_res['Spread_Savings_NOK'].sum()\n",
    "\n",
    "report = f\"\"\"\n",
    "FINAL REPORT: EXECUTION STRATEGY ANALYSIS (HARRIS THEORY)\n",
    "--------------------------------------------------------------\n",
    "1. THEORETICAL FRAMEWORK\n",
    "   - Tested \"Trader's Dilemma\": Paying for Immediacy (Market Orders) \n",
    "     vs. Risking Non-Execution for Price Improvement (Limit Orders).\n",
    "   - Focus: Reducing Transaction Costs (TCA) in the Summer Period.\n",
    "\n",
    "2. RESULTS (2020-2024)\n",
    "   - Average Cost (Aggressive): {avg_aggressive:.1f} bps\n",
    "   - Average Cost (Passive):    {avg_passive:.1f} bps\n",
    "   - Net Improvement (Alpha):   {avg_aggressive - avg_passive:.1f} bps\n",
    "\n",
    "3. ECONOMIC SIGNIFICANCE\n",
    "   - By using Passive Limit Orders instead of Aggressive Market Orders during\n",
    "     the summer, the investor captured a \"Liquidity Premium\".\n",
    "   - Total Estimated Savings (spread capture): {total_savings_nok:,.0f} NOK\n",
    "\"\"\"\n",
    "\n",
    "with open('output/Final_Execution_Report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "    \n",
    "print(report)\n",
    "print(\"Final execution strategy report saved to 'output/Final_Execution_Report.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3ec728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying FDR Correction (Benjamini-Hochberg) to OOS Results...\n",
      "Figures 12A & 12B saved to 'output' folder.\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 6: OUT-OF-SAMPLE VALIDATION (2020-2024)\n",
    "# FIGURE 12A & 12B: OUT-OF-SAMPLE VALIDATION (2020-2024)\n",
    "\n",
    "df_oos_full = df_full[df_full['Year'] >= 2020].copy()\n",
    "\n",
    "# Define Events (Same as Script 3)\n",
    "test_periods = ['pre_easter', 'post_easter',\n",
    "                'pre_christmas', 'christmas', 'post_christmas',\n",
    "                'summer_holiday', 'summer_excl_holiday']\n",
    "\n",
    "# Ensure we only test periods that actually exist in the OOS data\n",
    "available_periods_oos = df_oos_full['period_label'].unique()\n",
    "test_periods_oos = [p for p in test_periods if p in available_periods_oos]\n",
    "\n",
    "p_values_spread_oos = {}\n",
    "p_values_volume_oos = {}\n",
    "all_p_spread_oos = []\n",
    "all_p_volume_oos = []\n",
    "test_info_oos = []\n",
    "\n",
    "periods_for_heatmap_oos = ['control_year'] + test_periods_oos\n",
    "df_heatmap_oos = df_oos_full[df_oos_full['period_label'].isin(periods_for_heatmap_oos)].copy()\n",
    "\n",
    "# RUN T-TESTS \n",
    "for group in ['Large Cap', 'Mid Cap', 'Small Cap']:\n",
    "    p_values_spread_oos[group] = {}\n",
    "    p_values_volume_oos[group] = {}\n",
    "    \n",
    "    \n",
    "    control = df_heatmap_oos[\n",
    "        (df_heatmap_oos['period_label'] == 'control_year') & \n",
    "        (df_heatmap_oos['Cap_Group'] == group)\n",
    "    ]\n",
    "    \n",
    "    for period in test_periods_oos:\n",
    "        event = df_heatmap_oos[\n",
    "            (df_heatmap_oos['period_label'] == period) & \n",
    "            (df_heatmap_oos['Cap_Group'] == group)\n",
    "        ]\n",
    "        \n",
    "        if len(event) > 1 and len(control) > 1:\n",
    "            _, p_spread = ttest_ind(event['Spread'].dropna(), \n",
    "                                    control['Spread'].dropna(), \n",
    "                                    equal_var=False)\n",
    "            _, p_volume = ttest_ind(event['Volume'].dropna(), \n",
    "                                    control['Volume'].dropna(), \n",
    "                                    equal_var=False)\n",
    "            \n",
    "            all_p_spread_oos.append(p_spread)\n",
    "            all_p_volume_oos.append(p_volume)\n",
    "            test_info_oos.append((group, period))\n",
    "            \n",
    "            p_values_spread_oos[group][period] = p_spread\n",
    "            p_values_volume_oos[group][period] = p_volume\n",
    "        else:\n",
    "            p_values_spread_oos[group][period] = np.nan\n",
    "            p_values_volume_oos[group][period] = np.nan\n",
    "\n",
    "# FDR CORRECTION\n",
    "print(\"Applying FDR Correction (Benjamini-Hochberg) to OOS Results...\")\n",
    "\n",
    "reject_spread, pvals_corr_spread, _, _ = multipletests(all_p_spread_oos, alpha=0.05, method='fdr_bh')\n",
    "reject_vol, pvals_corr_vol, _, _ = multipletests(all_p_volume_oos, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "p_vals_spread_df_oos = pd.DataFrame(p_values_spread_oos).T\n",
    "p_vals_volume_df_oos = pd.DataFrame(p_values_volume_oos).T\n",
    "\n",
    "sig_matrix_spread_oos = pd.DataFrame(index=p_vals_spread_df_oos.index, columns=p_vals_spread_df_oos.columns)\n",
    "sig_matrix_volume_oos = pd.DataFrame(index=p_vals_volume_df_oos.index, columns=p_vals_volume_df_oos.columns)\n",
    "\n",
    "for i, (group, period) in enumerate(test_info_oos):\n",
    "    sig_matrix_spread_oos.loc[group, period] = pvals_corr_spread[i]\n",
    "    sig_matrix_volume_oos.loc[group, period] = pvals_corr_vol[i]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "sns.heatmap(p_vals_spread_df_oos.astype(float), annot=True, cmap='viridis_r', fmt=\".3f\", vmin=0, vmax=1,\n",
    "            linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (OOS 2020-2024)'})\n",
    "\n",
    "for i in range(len(sig_matrix_spread_oos.index)):\n",
    "    for j in range(len(sig_matrix_spread_oos.columns)):\n",
    "        val = sig_matrix_spread_oos.iloc[i, j]\n",
    "        if pd.notnull(val) and val < 0.05:\n",
    "            ax.add_patch(patches.Rectangle((j, i), 1, 1, fill=False, \n",
    "                                        edgecolor='red', lw=3))\n",
    "\n",
    "ax.set_title(\n",
    "    f'Out-of-Sample (2020-2024) P-values for Spread\\nRed border = Significant (FDR < 0.05)', \n",
    "    fontsize=16, fontweight='bold'\n",
    ")\n",
    "ax.set_xlabel('Holiday Events', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Market Cap Group', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_12A_OOS_PValues_Spread.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "sns.heatmap(p_vals_volume_df_oos.astype(float), annot=True, cmap='plasma_r', fmt=\".3f\", vmin=0, vmax=1,\n",
    "            linewidths=.5, ax=ax, cbar_kws={'label': 'P-value (OOS 2020-2024)'})\n",
    "\n",
    "for i in range(len(sig_matrix_volume_oos.index)):\n",
    "    for j in range(len(sig_matrix_volume_oos.columns)):\n",
    "        val = sig_matrix_volume_oos.iloc[i, j]\n",
    "        if pd.notnull(val) and val < 0.05:\n",
    "            ax.add_patch(patches.Rectangle((j, i), 1, 1, fill=False, \n",
    "                                        edgecolor='red', lw=3))\n",
    "\n",
    "ax.set_title(\n",
    "    f'Out-of-Sample (2020-2024) P-values for Volume\\nRed border = Significant (FDR < 0.05)', \n",
    "    fontsize=16, fontweight='bold'\n",
    ")\n",
    "ax.set_xlabel('Holiday Events', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Market Cap Group', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/Figure_12B_OOS_PValues_Volume.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Figures 12A & 12B saved to 'output' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "669425b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table saved as 'output/Table_Figure_2A_Stats_Both_Variances.png'\n"
     ]
    }
   ],
   "source": [
    "# SCRIPT 7: BONUS TABLE FOR FIGURE 2A (WITH BOTH VARIANCES)\n",
    "# TABLE FOR FIGURE 2A (WITH BOTH VARIANCES)\n",
    "\n",
    "summer_stats = df_summer.groupby(['period_label', 'Cap_Group'], observed=True)['Spread'].agg(\n",
    "    ['mean', 'count', 'std', 'var']\n",
    ")\n",
    "\n",
    "summer_stats['pop_var'] = summer_stats['var'] * (summer_stats['count'] - 1) / summer_stats['count']\n",
    "\n",
    "def get_ci(row):\n",
    "    t_crit = stats.t.ppf(0.975, row['count'] - 1)\n",
    "    margin_error = t_crit * (row['std'] / np.sqrt(row['count']))\n",
    "    return pd.Series([row['mean'] - margin_error, row['mean'] + margin_error], index=['CI_Lower', 'CI_Upper'])\n",
    "\n",
    "ci_df = summer_stats.apply(get_ci, axis=1)\n",
    "summer_table = pd.concat([summer_stats, ci_df], axis=1)\n",
    "\n",
    "output_table = summer_table[['mean', 'pop_var', 'var', 'CI_Lower', 'CI_Upper', 'count']].copy()\n",
    "output_table.columns = ['Mean', 'Pop. Variance (σ²)', 'Sample Var (S²)', '95% CI Lower', '95% CI Upper', 'N']\n",
    "\n",
    "period_totals = []\n",
    "for period in output_table.index.get_level_values('period_label').unique():\n",
    "    period_data_output = output_table.xs(period, level='period_label')\n",
    "    period_data_raw = df_summer[df_summer['period_label'] == period]['Spread']\n",
    "    \n",
    "    total_n = period_data_output['N'].sum()\n",
    "    weighted_mean = (period_data_output['Mean'] * period_data_output['N']).sum() / total_n\n",
    "    \n",
    "    pooled_variance = period_data_raw.var()\n",
    "    pop_variance = pooled_variance * (total_n - 1) / total_n\n",
    "    pooled_std = np.sqrt(pooled_variance)\n",
    "    \n",
    "    t_crit = stats.t.ppf(0.975, total_n - 1)\n",
    "    margin_error = t_crit * (pooled_std / np.sqrt(total_n))\n",
    "    ci_lower = weighted_mean - margin_error\n",
    "    ci_upper = weighted_mean + margin_error\n",
    "    \n",
    "    period_totals.append((period, 'All Cap', weighted_mean, pop_variance, pooled_variance, ci_lower, ci_upper, total_n))\n",
    "\n",
    "period_totals_df = pd.DataFrame(period_totals, columns=['period_label', 'Cap_Group', 'Mean', 'Pop. Variance (σ²)', 'Sample Var (S²)', '95% CI Lower', '95% CI Upper', 'N'])\n",
    "period_totals_df = period_totals_df.set_index(['period_label', 'Cap_Group'])\n",
    "\n",
    "output_table_regular = output_table.copy()\n",
    "output_table_totals = period_totals_df.copy()\n",
    "\n",
    "output_table_display_regular = output_table_regular.copy()\n",
    "output_table_display_regular['Mean'] = output_table_display_regular['Mean'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_regular['Pop. Variance (σ²)'] = output_table_display_regular['Pop. Variance (σ²)'].apply(lambda x: f\"{x:.8f}\")\n",
    "output_table_display_regular['Sample Var (S²)'] = output_table_display_regular['Sample Var (S²)'].apply(lambda x: f\"{x:.8f}\")\n",
    "output_table_display_regular['95% CI Lower'] = output_table_display_regular['95% CI Lower'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_regular['95% CI Upper'] = output_table_display_regular['95% CI Upper'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_regular['N'] = output_table_display_regular['N'].apply(lambda x: f\"{int(x):,}\")\n",
    "output_table_display_regular = output_table_display_regular.reset_index()\n",
    "\n",
    "output_table_display_totals = output_table_totals.copy()\n",
    "output_table_display_totals['Mean'] = output_table_display_totals['Mean'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_totals['Pop. Variance (σ²)'] = output_table_display_totals['Pop. Variance (σ²)'].apply(lambda x: f\"{x:.8f}\")\n",
    "output_table_display_totals['Sample Var (S²)'] = output_table_display_totals['Sample Var (S²)'].apply(lambda x: f\"{x:.8f}\")\n",
    "output_table_display_totals['95% CI Lower'] = output_table_display_totals['95% CI Lower'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_totals['95% CI Upper'] = output_table_display_totals['95% CI Upper'].apply(lambda x: f\"{x:.5f}\")\n",
    "output_table_display_totals['N'] = output_table_display_totals['N'].apply(lambda x: f\"{int(x):,}\")\n",
    "output_table_display_totals = output_table_display_totals.reset_index()\n",
    "\n",
    "output_table_display = pd.concat([output_table_display_regular, output_table_display_totals], ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6)) \n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=output_table_display.values, \n",
    "                colLabels=output_table_display.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.12, 0.12, 0.12, 0.14, 0.14, 0.12, 0.12, 0.08])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "for i in range(len(output_table_display.columns)):\n",
    "    table[(0, i)].set_facecolor('#3498db')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(output_table_display) + 1):\n",
    "    if i % 2 == 0:\n",
    "        for j in range(len(output_table_display.columns)):\n",
    "            table[(i, j)].set_facecolor('#ecf0f1')\n",
    "\n",
    "num_regular_rows = len(output_table_display_regular)\n",
    "for i in range(num_regular_rows + 1, len(output_table_display) + 1):\n",
    "    for j in range(len(output_table_display.columns)):\n",
    "        table[(i, j)].set_text_props(weight='bold')\n",
    "\n",
    "plt.title('Table: Descriptive Statistics (Comparing Variances) for Figure 2A', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/Table_Figure_2A_Stats_Both_Variances.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Table saved as 'output/Table_Figure_2A_Stats_Both_Variances.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
